the following is a conversation with edward gibson or ted as everybody calls him he is a psycholinguistics professor at mit he heads the mit language lab that investigates why human languages look the way they do the relationship between culture and language and how people represent process and learn language also he should have a book titled syntax a cognitive approach published by mit press coming out this fall so look out for that and now a quick few second mention of each sponsor check them out in the description it's the best way to support this podcast we got yahoo finance for basically everything you've ever needed if you're an investor listening for listening to research papers policy genius for insurance shopify for selling stuff online and eight sleep for naps she was wise to my friends also if you want to work with our amazing team or just get in touch with me go to alex friedman dot com slash contact and now onto the full ad reads as always no ads in the middle i try to make this interesting but if you must skip friends please do check out the sponsors i enjoyed their stuff maybe you will too this episode is brought to you by yahoo finance a new sponsor and they got a new website that you should check out it's a website that provides financial management reports information and news for investors yahoo itself has been around forever yahoo finance has been around forever i don't know how long but it must be over twenty years it survived so much it evolved rapidly and quickly adjusting evolving improving all of that the thing i use it for now is there's a portfolio that you can add your account to ever since i had zero money i used boy i think it's called td ameritrade i still use that same thing just getting a basic mutual fund and i think td ameritrade got bought by charles schwab or acquired or merged i don't know i don't know how these things work all i know is that yahoo finance can integrate that and just show me everything i need to know about my quote unquote portfolio i don't have anything interesting going on but it is still good to kinda monitor it to stay in touch now a lot of people i know have a lot more interesting stuff going on investment wise so all of that could be easily integrated into yahoo finance and you could look at all that stuff the charts blah blah blah it looks beautiful and sexy and just helps you be informed now that's about your own portfolio but then also for the entirety of the finance information for the entirety of the world that's all there the big news the analysis of everything that's going on everything like that and i should also mention that i would like to do more and more financial episodes i've done a couple of conversations with ray dalio a lot of that is about finance but some of that is about sort of geopolitics and the bigger context of finance i just recently did a conversation with bill ackman very much about finance and i did a series of conversations on cryptocurrency lots and lots of brilliant people michael sailor so on charles hoskinson vitalik i mean just lots of brilliant people in that space thinking about the future of money future of finance anyway you can keep track of all of that with yahoo finance for comprehensive financial news and analysis go to yahoo finance dot com that's yahoo finance dot com this episode is also brought to you by listening an app that allows you to listen to academic papers it's a thing i've always wished existed and i always kind of suspected it's very difficult to pull off but these guys pulled it off basically it's any kind of formatted text brought to life through audio now for me the thing i care about most and i think that's at the foundation of listening is academic papers so i love to read academic papers and there's several levels of rigor in the actual reading process but listening to them especially after i skimmed it or after i did a deep dive listening to them is just such a beautiful experience it solidifies the understanding it brings to life all kinds of thoughts and i'm doing this while i'm cooking while i'm running while i'm going to grab a coffee all that kind of stuff it does require an elevated level of focus especially the kind of papers i listen to which are computer science papers but you can load in all kinds of stuff you can do philosophy papers you could do psychology papers like this very topic of linguistics i've listened to a few papers on linguistics i went back to chomsky and listened to papers it's great papers books pdfs web pages articles all that kind of stuff even email newsletters and the voices they got are pretty sexy it's great it's pleasant to listen to i think that's what's ultimately most important is it shouldn't feel like a chore to listen to it like i really enjoy it normally you'd get a two week free trial but listeners of this podcast get one month free so go to listening dot com slash lex that's listening dot com slash lex this episode is brought to you by policygenius a marketplace for insurance life auto home disability all kinds of insurance there's really nice tools for comparison i'm a big fan of nice tools for comparison like i have to travel to harsh conditions soon and i had to figure out how i need to update my equipment to make sure it's weatherproof waterproof even it's just resilient to harsh conditions and it would be nice to have sort of comparisons i have to resort to like reddit posts or forum posts kind of debating different audio recorders and cabling and microphones and and waterproof containers all that kind of stuff i would love to be able to do like a rigorous comparison of them of course going to amazon you get the reviews and those are actually really really solid and so i think amazon's been the giant gift to society in that way that you kinda can lay out all the different options and get a lot of structured analysis of how good this thing is so amazon has been great at that now what policygenius did is did the the amazon thing but for insurance so the the tools for comparison is really my favorite thing it's just really easy to understand the full marketplace of insurance with policygenius you can find life insurance policies that start at just two hundred and ninety two dollars per year for one million dollars of coverage at the policy genius dot com slash lux or click the link in the description to get your free life insurance quotes and see how much you can save that's policy genius dot com slash lex this episode is also brought to you by shopify a platform designed for anyone to sell anywhere with a great looking online store i'm not name dropping here but i recently went on a hike with the ceo of shopify doby he's brilliant i've i've been a fan of his for a long time long before shopify was a sponsor i don't even know if he knows that shopify sponsors this podcast now just to clarify it really doesn't matter nobody in this world can put pressure on me to have a sponsor or not to have sponsor or for a sponsor to put pressure on me what i can and can't say i when i wake up in the morning feel completely free to say what i wanna say and to think what i wanna think i've been very fortunate in that way in many dimensions in my life and i also have always lived a frugal life and a life of discipline which is where the freedom of speech and the freedom of thought truly comes from so i don't need anybody i don't need a boss i don't need money i'm free to exist in this world in the way i see is right now on top of that of course i'm surrounded by incredible people many of whom i disagree with and have arguments so i'm influenced by those conversations and those arguments and i'm always learning always challenging myself always humbling myself i have kind of intellectual humility i kinda suspect i'm kind of an idiot i start my approach to the world of ideas from that place assuming i'm an idiot and everybody has a lesson to teach me anyway not sure why i got on off that tangent but the hike looked beautiful nature friends is beautiful anyway i have a shopify store lex freeman dot com slash store it's very minimal which is how i like i think most things if you wanna set up a store it's super easy it takes a few minutes even i figured out how to do it sign up for a one dollar per month trial period at shopify dot com slash lex that's all lowercase go to shopify dot com slash lex to take your business to the next level today this episode is also brought to you by eight sleep and its pod three cover the source of my escape the door when opened allows me to travel away from the troubles of the world into this ethereal universe of calmness a cold bed surface with a warm blanket a perfect twenty minute nap and it doesn't matter how dark the place my mind is in a nap will pull me out and i see the beauty of the world again technologically speaking a sleep is just really cool you can control temperature with an app it's become such an integral part of my life that i've begun to take it for granted typical human so the the app controls the temperature i set it currently i'm setting it to a a negative five and it's just super nice cool surface it's something i really look forward to especially when i'm traveling i don't have one of those it really makes me feel like home check it out and get special savings when you go to eight sleep dot com slash lex this is a lex freeman podcast to support it please check out our sponsors in the description and now dear friends here's edward gibson when did you first become fascinated with human language as a kid in school when we had to structure sentences in english grammar i i i found that process interesting i found it confusing as to what it was i was told to do i didn't didn't understand what the theory was behind it but i found it very interesting so when you look at grammar you're almost thinking about it like a puzzle like almost like a mathematical puzzle yeah i think that's right i didn't know i was gonna work on this at all at that point i was really just i was kind of a math geek person like computer scientist i really liked computer science and then i found language as a a neat puzzle to work on from an engineering perspective actually it's what i and and as a i i sort of accidentally fly i decided after i finished my undergraduate degree which was computer science and math in canada and queen's university i decided to go to grad school as i that's what i always thought i would do and i went to cambridge where they had a master's in a master's program in computational linguistics and i hadn't taken a single language class before all i had taken was cs computer science math classes pretty much mostly as an undergrad and i just thought this was an interesting thing to do for a year because it was a single year program and then i end up spending my whole life doing it so fundamentally your journey through life was one of a mathematician and computer scientist and then you kinda discovered the puzzle the problem of language and approached it from that angle to try to understand it from that angle almost like a mathematician or maybe even an engineer as an engineer i'd say i mean to be frank i had taken an ai class i guess it was eighty three or eighty four eighty five somewhere eighty four in there a long time ago mhmm and there was a natural language section in there and it didn't impress me i thought there must be more interesting things we can do didn't it didn't seem very it seemed just a bunch of hacks to me it didn't seem like a real theory of things in any way and so i just thought this was this seemed like an interesting area where there wasn't enough good work did you ever come across like the the philosophy angle of logic so if you think about the eighties with ai the expert systems where you try to yeah kinda maybe sidestep the the poetry of language and the some of the the syntax and the grammar and all that kind of stuff and go to the underlying meaning that language is trying to communicate and try to somehow compress that in a computer representable way do you ever come across that in your studies i mean i probably did but i wasn't as interested in it i was i was trying to do the easier problems first the ones i could thought maybe were handleable which is seems like the syntax is easier like which is just the forms as opposed to the meaning like you're talking about when you're starting talking about the meaning that's very hard problem i and it still is a really really hard problem but but the forms is is easier and so i thought at least figuring out the forms of human language which sounds really hard but it's actually maybe more tractable so it's interesting you think there is a big divide there's a gap there's a distance between form and meaning because that's a question you have discussed a lot with llms mhmm because they're damn good at form yeah i think that's what they're good at is form yeah exactly and that's that's why they're good because they can do form meaning's hard do you think there's oh wow and i mean it's an open question right yeah how close form and meaning are yeah we'll discuss it but i to me studying form maybe it's a romantic notion mhmm gives you form is like the shadow of the the bigger meaning thing underlying language as i it form is is language is how we communicate ideas we communicate with each other using language so in understanding the structure of that communication i think you start to understand the structure of thought and the structure of meaning behind those thoughts and communication to me but to you big gap yeah what do you find most beautiful about human language maybe the form of human language the expression of human language what i find beautiful about human language is the some of the generalizations mhmm that happen across human languages within and across a language so let me give you an example of something which i find kind of remarkable that is if like a language if it has a a word order such that the verbs tend to come before their objects and so that's like english does that so we have the the first the subject comes first in a in a simple sentence so i say you know the the dog chased the cat or mary kicked the ball so the subject's first the and then after the subject there's the verb and then we have objects all these things come after in english so it's a it's generally a verb and most of the stuff that we wanna say comes after the subject it comes it's the it's the objects there's a lot of things we wanna say come after and and and there's a lot of languages like that about forty percent of the languages of the world are look like that they're sub subject verb object languages and then these languages tend to have prepositions these little markers on the nouns that that connect nouns to other nouns or nouns to verbs so i when i so a verbal i sorry preposition like in or on or of or about i say i talk about something the something is the object of that preposition that we have these little markers come also just like verbs they come before their their nouns okay and then so now we look at other languages that like japanese or or hindi or some these are these are so called verb final languages those is about maybe a little more than forty percent maybe forty five percent of the world's languages or or more i mean fifty percent of the world's languages are verb final those tend to be postpositions those markers the state we have the states have the same kinds of markers as we do in english but they put them after so sorry they they put them first the markers come first so you say instead of you know talk about a book you say book about the opposite order there in japanese or in hindi you do the opposite and the and the talk comes at the end so the verb will come at the end as well so instead of mary kicked the ball it's mary ball kicked and then if if it says mary kicked the ball to john it's john to the to the little the marker there the preposition cut it's a postposition in these languages and so the interesting thing a fascinating thing to me is that within a language this order aligns it's harmonic i and so if it's one or the other it's either verb initial or verb final but then you'll then you'll have prepositions prepositions or postpositions and so that and that's across the languages that we we can look at we got around a thousand languages for for there's around seven thousand languages around on the on the earth right now but we have information about say word order on around a thousand of those for pretty decent amount of information and for those thousand which we know about about ninety five percent fit that pattern so they will have either verb it's about it's about half and half or half of verb initial like english and half for verb final like like japanese so just to clarify verb initial is subject verb object that's correct verb final is still subject object verb that's correct yeah the subject is generally first that's so fascinating i ate an apple or i apple ate yes okay and it's fascinating that there's a pretty even division in the world amongst those forty forty five percent yeah it's pretty it's pretty even and and those two are the most common by far those two word errors the subject tends to be first there's so many interesting things but these things are one thing i find so fascinating is there are these generalizations within and across a language and and not only those are the the and there's actually a simple explanation i think for a lot of that and that is you you're trying to like minimize dependencies between words that's basically the story i think behind a lot of why word order looks the way it is is you we're always connecting what is it what is the thing i'm telling you i'm i'm talking to you in sentences you're talking to me in sentences these are sequences of words which are connected and the connections are dependencies between the words and it turns out that what we what we're trying to do in a language is actually minimize those dependency lengths it's easier for me to say things if the words that are connecting for their meaning are close together it's easier for you in understanding if that's also true if they're far away it's it's hard as it to produce produce that and it's hard for you to understand and the languages of the world within a language and across languages you know fit that generalization which is you know so i i you know it turns out that having verbs initial and then having prepositions ends up making dependencies shorter and and having verbs final and having postpositions ends up making dependencies shorter than if you cross them if you cross them it ends up you just end up it's possible you can do it it just within a language within a language you can do it it just ends up with longer dependencies than if you didn't and so languages tend to go that way they tend to minimally this is they said they call it harmonic so it was observed a long time ago by without the explanation by a guy called joseph greenberg who's a famous typologist from stanford he observed a lot of generalizations about how word order works and these are some of the harmonic generalizations that he observed harmonic generalizations about word word order there's so many things i wanna ask you okay let me just sometimes basics you you you mentioned dependencies a few times yeah what do you mean by dependencies well what i mean is in in language there's kind of three structures to three components to the structure of language one is the sounds so cat is k a and t in english i'm not talking about that part i'm talking then there's two meaning parts and those are the words and and you're talking about meaning earlier so words have a form and they have a meaning associated with them and so cat is a full form in english and it has a meaning associated with whatever cat is and then the combinations of words that's what i'll call grammar or syntax and that's like when i have a combination like the cat or two cats okay so where i take two different words there and put them together and i get a compositional meaning from putting those two different words together and and so that's the syntax and in any sentence or utterance whatever i'm talking to you you're talking to me we have a bunch of words and we're putting together in a sequence they it turns out they are connected so that every word is connected to just one other word in that in that sentence mhmm and so you end up with what's what's called technically a tree it's a tree structure so there where there's a root of that of that utterance of that sentence and then there's a bunch of dependence like branches from that root that go down to the words the words are the leaves in this metaphor for a tree so tree is also sort of a mathematical construct yeah it's a graph theoretical thing exactly theory thing yeah yeah so and and it's fascinating that you can break down a sentence into a tree and then one every word is hanging on to another this depending on it and and everyone agrees on that so all linguists will agree with that no one this is not a controversial that is not controversial there's nobody sitting here listening not think so mad at you i don't think so okay there's no linguist sitting there mad at this no i think in every language i think everyone agrees that all sentences are trees at some level can i pause on that sure because it it's to me just as a layman it it's surprising yeah that you can break down sentences in many mostly all languages all languages i think into a tree i think so that's weird i i've never heard of anyone disagreeing with that that's weird the details of the trees are what people disagree about well okay so what's what what's at the root of a tree how do you construct how hard is it what is the process of constructing a tree from a sentence well this is where you know depending on what your there's different theoretical notions i'm gonna just say the simplest thing dependency grammar it's like a bunch of people invented this was the first french guy back in i mean the paper was published in nineteen fifty nine but he was working on the thirties and stuff so and and it goes back to you know philologist pagnini was doing this in ancient india okay and so you know doing something like this the simplest thing we can think of is that there's just connections between the words to make the the utterance and so let's just say i have like two dogs entered a room okay here's a sentence and so we're connecting two and dogs together mhmm that's like there's some dependency between those words to make some bigger meaning and then we're connecting dogs now to entered mhmm right and we connect a room somehow to entered and so i'm gonna connect to room and then room back to entered this is that's the tree is i the the root is entered that's the the thing is like an entering event that's what we're saying here and the the subject which is whatever that dog is is two dogs it was and and the connection goes back to dogs which goes back to then then that that goes back to two i'm just that that's my tree it it starts at entered goes to dogs down to two and on the other side after the verb the object it goes to room and then that goes back to the the determiner or article whatever you wanna call that word so there's a bunch of categories of words here we're noticing so there are verbs those are these things that typically mark they refer to events and states in the world and they're nouns which typically refer to people places and things is what people say but they can refer to other more they can refer to events themselves as well they're they're they're marked by you know how they how they get you what what the category the part of speech of a word is how it gets used in language it's like that's how you decide what the what the category of a word is not not by the meaning but how it's how it gets used how it's used yeah what's usually the root is it gonna be the verb that defines the event usually yes yes okay yeah i mean if i don't say a verb then there won't be a verb until it'll be something else what if you're messing are we talking about language that's like correct language what if you're doing poetry and messing with stuff is it then then rules go out the window right then it's no you're still no no no no no you're constrained by whatever language you're dealing with probably you have other constraints in poetry such that you're like usually in poetry there's multiple constraints that you want to like you wanna usually convey rhyming structure as well and depending on so but you usually are constrained by your the rules of your language for the most part and so you don't violate those too much you can violate them somewhat but not too much so it has to be recognizable as your language like in english i can't say dogs to entered room a i mean i meant the you know two dogs entered a room and i i i can't mess with the order of the the articles the articles and the nouns you just can't do that in some languages you can you can mess around with the order of words much more i mean you speak russian mhmm russian has a much freer word order than english and so in fact you can move around words in you know i told you that english has the subject verb object word order so does russian but russian is much freer than english and so you can actually mess around with the word order so probably russian poetry is gonna be quite different from english poetry because the word order is much less constrained yeah there's a much more extensive culture of poetry throughout the history of the last hundred years in russia and i i was always wondered why that is but it seems that there's more flexibility in the way the language is used there's more you're morphing the language easier by altering the words altering the order of the words messing with it well you can just mess with different things in each language and so in russian you have case markers right on the end which is these these endings on the nouns which tell you how it connects each noun connects to the verb right we don't have that in english and so when i say mary kissed john i don't know who the agent or the patient is except by the order of the words right in in russian you actually have a marker on the end if you're using a russian name in each of those names you'll also say is it you know agent it'll be the you know of nominative which is marking the subject or an accusative will mark the object and you could put them in the reverse order you could put accusative first as you could put subject you could put the patient first and then the verb and then the the the subject and that would be a perfectly good russian sentence and would still mean mary i could say john kissed mary meaning mary kissed john as long as i use the case markers in the right way you can't do that in english and so i love the terminology of agent and patient and yeah and the other ones you used well those are sort of linguistic terms correct those are those are for like kinda meaning those are meaning and and subject and object are generally used for position so subject is just like the thing that comes before the the verb and the object is the one that comes after the verb the agent is kinda like the thing doing it that's kinda what that means right the the the subject is often the person doing the action right the thing so yeah yeah okay this is fascinating so how hard is it to form a tree in general is there is there a procedure to it like if you look at different languages is it supposed to be a very natural like is it automatable or is there some human genius involved in this i think it's pretty automatable at this point people can figure out the words are they can figure out the morphemes which are the technically morphemes are the the minimal meaning units within a language okay and so when you say eats or drinks it actually has two morphemes in it in english there's there's the there's the root which is the verb and then there's some ending on it which tells you you know that's this third person third person singular say what morphisms are morphemes are just the minimal meaning units within a language and a word is just kinda the things we put spaces between english and they've got a little bit more they have the morphology as well they have the endings this inflectional morphology on the endings on the roots it modifies something about the word that yeah adds additional meaning they tell you yeah yeah yeah and so we have a little bit of that in english very little you have much more in russian yeah for instance and and but we have a little bit in english and so we have a little on the on the nouns you can say it's either singular or plural and and you can say same thing for for for verbs like simple past tense for example so like you know notice in english we say drinks you know he drinks but everyone else is i drink you drink we drink it's unmarked in a in a way and then but in the past tense it's just drank there for everyone there's no morphology at all for past tense it's if there is morphology it's marking past tense but it's kind of it's an irregular now so we don't even you know drink to drink you know it's not even a regular word so in most verbs many verbs there's an e d we kinda add so walk to walk we add that to say it's the past tense that i just happened to choose an irregular because the high frequency word and the high frequency words tend to have irregulars in english for what's an irregular that's irregular is just a there's there isn't a rule so drink to drink is an is an irregular drink drank okay k well first of all to walk walked talked yeah talked and there's a lot of irregulars in english there's a lot of irregulars in english the the the frequent ones the common words tend to be irregular the there's many many more low frequency words yeah and those tend to be those are regular ones the evolution of the irregulars are fascinating because it's essentially slang that's sticky mhmm because you're breaking the rules and then everybody use it and doesn't follow the rules yeah and they they say screw it to the rules yeah it's fascinating so you said that morphemes lots of questions so morphology is what the study of morphemes morphology is the is the connections between the morphemes onto the roots the roots so so in english we mostly have suffixes we have end endings on the words not very much but a little bit and as opposed to prefixes some words depending on your language can have you know mostly prefixes mostly suffixes or mostly or or both and then even languages several languages have things called infixes where you have some kind of a general form for the for the root and you put stuff in the middle you change the vowels that's fascinating yeah that's fascinating so wait so in general there's what two morphemes per word usually one or two or three well in english it's it's it's one or two in english it tends to be one or two there can be more you know in in other languages you know a language language like like finnish which has a very elaborate morphology there may be ten morphemes on the end of a root okay and so there may be mill there'd be millions of forms of a given word okay okay so i i will ask the same question over and over but how does the just sometimes to understand mhmm things like morphemes it's nice to just ask the question how does these kinds of things evolve so you have a great book studying sort of the how how the cognitive processing how language used for communication so the the mathematical notion of how effective language is for communication and what role that plays in the evolution of language but just high level like how do we how does a language evolve with where english has two morphemes or one or two morphemes per word and then finnish has infinity per word so what how does that how does that happen is it just peep people that's a really good question yeah that's a very good question it's like why do languages have more morphology versus less morphology and and i don't think we know the answer to this i don't i think there's just like a lot of good solutions to the problem of communication so i like i believe as you hinted that language is an invented system by humans for communicating their ideas and i think we it comes down to we label the things we wanna talk about those are the the the morphemes and words those are the things we wanna talk about in the world and we invent those things and then we put them together in ways that are easy for us to convey to process but that that that's like a naive view and i don't i mean i i think it's probably right right it's naive and probably right well that's the i don't know if it's naive i think it's simple simple yeah i think naive is okay okay the naive is an indication that it's an incorrect somehow it's a trivial yeah too too simple and i think it could very well be correct yeah but it's interesting how sticky it feels like two people got together it just it just feels like once you figure out certain aspects of a language they just become sticky and the tribe forms around that language maybe the language maybe the tribe forms first then the language evolves and then you just kind of agree and then you stick to whatever that is i mean these are very interesting questions we don't know yeah really about how words even words get invented very much about you know we don't really i mean assuming they get invented they we don't really know how that process works and how these things evolve what we have is kind of a a current picture a current picture of a few thousand languages a few thousand instances we don't have any pictures of really how these things are evolving really and and then the evolution is massively you know confused by contact right so as soon as one language group one group runs into another we are smart humans are smart and they take on whatever is useful in the other group and so any kind of contrast which you're talking about which i find useful i'm gonna i'm gonna start using as well so i i worked a little bit in in in specific areas of words in in number words and in in color words and in color words that so we have in english we have around eleven words that everyone knows for colors and and many more if you happen to be interested in color for some reason or other if you're a fashion designer or an artist or something you may have many many more words but we can see millions like if you have normal color vision normal trichrometric color vision you can see millions of words you know the most efficient no so the most you know detailed color vocabulary would have over a million terms to distinguish all the different colors that we can see but of course we don't have that so it's somehow it's been it's kind of useful for english to have have evolved in some way to so there's there's eleven terms that people find useful to talk about you know black white red blue green yellow purple gray pink and i probably missed something there anyway there's there's eleven that everyone knows yeah and and depending on your and but you go to different cultures especially the non industrialized cultures and there'll be many fewer so some cultures will have only two believe it or not that the danai in in papua new guinea have only two labels that the that the group uses for color those are roughly black and white they are k very very dark and very very light which are roughly black and white and you might think oh they're dividing the whole color space into you know light and dark or something and that's not really true they mostly just only label the light the black and the white things they just don't talk about the colors for the other ones and so and and then there's other groups i worked with a group called the tsimane down in in bolivia in south america and they have three words that everyone knows but there's a few others that are that that several people that many people know and so they have may it's just kinda depending on how you count between three and seven words that the group knows okay and and again they're they're black and white everyone knows those and red red is you know you know like that tends to be the third word that everyone that that cultures bring in nice if there's a word it's always red the third one and then after that it's kind of all bets are off about what they bring in and so after that they they bring in a sort of a big blue green spay grew they have one for that and then they and then you know different people have different words that they'll use for other parts of the space and so yeah yeah anyway it's and it's probably related to what they wanna talk what they not what they not what they see because they see the same colors as we see so it's not like they have they don't they have a a a weak a low color palette in the things they're looking at they're looking at a lot of beautiful scenery mhmm a lot of different colored flowers and berries and things and you know and so there's lots of things of very bright colors but they just don't label the color in those cases and the reason probably we we don't know this but we think probably what's going on here is that what you do why you label something is you need to talk to someone else about it and and why do i need to talk about a color well if i have two things which are identical and i want you to give me the one that's different and and the only way it varies is color then i invent a word which tells you you know this is the one i want so i want the red sweater off the rack not the not the green sweater right there's two and and so those those things will be identical ex because these are things we made and they're dyed and there there's nothing different about them and so in in industrialized society we have you know everything everything we've got is pretty much arbitrarily colored but if you go to a non industrialized group that's not true and so they don't it's not only they're not interested in color if you you bring bright colored things to them they like them just like we like them bright colors are great they're beautiful they are but they just don't need to no need to talk about them they don't have so probably color words is a good example of how language evolves from sort of function when you need to communicate the use of something i think so then then you kind of invent different variations and and basically you can imagine that the evolution of a language has to do with what the early tribe's doing like what what they wanted what what kind of problems are facing them and they're quickly figuring out how to efficiently communicate the solution to those problems whether it's aesthetic or functional all that kind of stuff running away from a mammoth or whatever but you know it's so so i think what you're pointing to is that we don't have data on the evolution of language because many languages are formed a long time ago so you don't get the chatter we have a little bit of like old english to modern english because there was a writing system and we can see how how old english looked so the word order changed for instance old english to middle english to modern english and so it it you know we could see things like that but most languages don't even have a writing system so of the seven thousand only you know a small subset of those have a writing system and even if they have a writing system they it's not a very modern writing system so they don't have it so we just basically have for mandarin for chinese we have a lot of a lot of evidence from from for a long time and for english and not for much else not for men in german a little bit but not for a whole lot of like long term language evolution we don't have a lot we just have snapshots is what we've got of current languages yeah i i you get an inkling of that from the rapid communication on certain platforms like on reddit oh yeah there's different communities mhmm and they'll come up with different slang usually from my perspective durham by a little bit of humor or maybe mockery or whatever it is you know just talking shit in different kinds of ways and you could see the evolution of language there because i think a lot of things on the internet you don't want to be the boring mainstream so you like want to deviate from the proper way of talking mhmm and so you get a lot of deviation like rapid deviation then when communities collide you get like just like you said humans adapt to it and you could see it through the lines of humor i mean it's very difficult to study but you can imagine like a hundred years from now well if there's a new language born for example we'll get really high resolution data i mean in english is changing in english changes all the time all languages change all the time so you know there's a famous result about the queen's english so the queen if you look at the queen's vowels the queen's english is supposed to be you know originally the proper way for the talk was sort of defined by whoever the queen talked or the king whoever was in charge and and and so if you look at the how her vowels changed from when she first became queen in nineteen fifty two or fifty three when she was coronated the first i mean that's queen elizabeth who's got who who died recently of course until you know fifty years later her vowels changed her vowels shifted a lot mhmm and so that you know even in the sounds of british english in her her the way she was talking was changing the vowels were changing slightly so that's just in the sounds there's change i don't know what's you know we're we're i'm interested we're all interested in what's driving any of these changes the the word order of english changed a lot over thousand years right so it used to look like german you know it looks it used to be a verb final language with case marking and it shifted to a remedial language a lot of contact so a lot of contact with french and it became a remedial language with no case marking and so it became this you know verb verb initially thing so and so that's it's evolving we it it totally evolved and so it may very i mean you know it doesn't evolve maybe very much in twenty years is maybe what you're talking about but over fifty and a hundred years things change a lot i i think we'll now have good data on it yeah which is great for sure yeah can you talk to what is syntax and what is grammar so you wrote a book on syntax i did you were asking me before about what you know how do i figure out what a dependency structure is and i'd say the dependency structures aren't that hard generally i think there's a lot of agreement of what they of what they are for almost any sentence in in most languages i think people will agree on a lot of that there are other parameters in the mix such that some people think there's a more complicated grammar than just a dependency structure and so you know like noam chomsky he's the most famous linguist ever and he he is famous for proposing a a a slightly more complicated syntax and so he he invented phrase structure grammar so he's well known for many many things but in the fifties and in early sixties like but late fifties he was basically figuring out what's called formal language theory mhmm so and he figured out sort of a a framework for figuring out how complicated language you know a certain type of language might be so called phrase structure grammars of language might be and so he his his idea was that maybe we can we can think about the complexity of a language by how complicated the rules are okay and the rules will look like this they will have a left hand side and they'll have a right right hand side look something will on the left hand side will expand to the thing on the right hand side so we'll say we'll start with an an s which is like the root which is an a sentence okay and then we're gonna expand to things like a noun phrase and a verb phrase is what he would say for instance okay an s goes to an np and a vp is a kind of a phrase structure rule and then and we figure out what an np is an np is a an a determiner and a noun for instance and a verb verb phrase is something else as a verb and another noun phrase and another np for instance those are the rules of a very simple phrase structure mhmm okay and and so he he proposed phrase structure grammar as a way to sort of cover human languages and then he actually figured out that well depending on the formalization of those grammars you might get more complicated or less complicated languages so you could he could he said well you these are these are things called you know context free languages that rule that he thought you know human languages tend to be what he calls context free languages and the but there are simpler languages which are so called regular languages and they have a more a more constrained form to the rules of the of the phrase structure of of these particular rules so he he basically discovered and kind of invented ways to describe the language and those and those are phrase those are phrase structure a a a human language and he was mostly interested in english initially in his his work in the fifties so quick questions around all of this so formal language theory is the big field of just studying language formally yes and it doesn't have to be human language there we have computer languages any kind of system which is generating a some set of expressions in a language and those could be like the the you know the statements in a in a computer language for example so formal it could be that or it could be human language so technically you can study programming language yes and have been i mean heavily studied using this formalism there there's a big field of programming languages within the formal language okay and then phrase structure grammar is this idea that you can break down language into this snp vp yeah like type of thing it's it's a particular formalism for describing language back in the fifties and and and but he and and that's equivalent actually the this the context free grammar is actually is kind of equivalent in the sense that it generates the same sentences as a dependency grammar would you know as the the dependency grammar is a little simpler in some way you just have a root and it goes like we don't have any of these the the rules are implicit i guess in in we just have connections between words the free structure grammars are kind of a different way to think about the the dependency grammar so it's slightly more complicated but it's kinda the same in some ways so to clarify dependency grammar is the framework under which you see language and you make a case that this is a good way to describe language that's correct and noam noam chomsky is watching this is very upset right now so let's just kidding but what's the difference between where's the the place of disagreement between phrase structure grammar and dependency grammar they're they're very close so phrase structure grammar and dependency grammar aren't that aren't that far apart i i i like dependency grammar because it's more perspicuous it's more transparent about representing the connections between the words it's just a little harder to see in phrase structure grammar you know the the place where chomsky sort of devolved or went off from from from this is he also thought there was something called movement okay and so so and that's where we disagree okay that's the place where i would say we disagree and and and i mean well maybe we'll get into that later but the idea is if you wanna do you want me to explain that in order i would love can you explain movement movement okay so you're saying so many interesting things yeah yeah yeah okay so here's the movement is chomsky basically sees english and he says okay i said you know this is we had that sent sentence early like it was like two dogs entered the room it's changed a little bit say two dogs will enter the room and he notices that hey english if i wanna make a question a a yes no question from that same sentence i i say instead of two dogs will enter the room i say will two dogs enter the room okay there's a different way to to say the same idea and it's like well the auxiliary verb that will thing it's at the front as opposed to in the middle okay and so and he looked you know if you look at english you see that that's true for all those modal verbs and for other kinds of auxiliary verbs in english you always do that you always put an auxiliary verb at the front and and what he's when he saw that so you know if i say i can win this bet can i win this bet right so i move a can to the front so actually that's a theory i i just gave you a theory there i he he talks about it as movement that word in the decla thinks the declarative is the root is is the sort of default way to think about the sentence and you move the auxiliary verb to the front that's a movement theory okay so and he he just thought that was just so obvious that it must be true that that that there's nothing more to say about that that this is how auxiliary verbs work in english there's a movement rule such that your move like to get from the declarative to the interrogative you're moving the auxiliary to the front and it's a little more complicated as soon as you go to simple temp simple present and simple past because you know if i say you know john slept you have to say did john sleep not slept john right and so it's you you have to somehow get an auxiliary verb and i guess underlyingly it's like slept is it's a little more complicated than that but his that's his idea there's a movement okay and and and so a different way to think about that that isn't i mean the then then he ended up showing later so he proposed this theory of grammar which has movement there's other places where he thought there's movement not just auxiliary verbs has movement there's other places where he thought there's movement not just auxiliary verbs but things like the passive in english and things like questions wh questions a bunch of places where he thought there's also movement going on and and in each each one of those he thinks there's words well phrases and words are moving around from one structure to another which he called deep structure to surface structure i mean there's like two different structures in his in his theory okay there's a different way to think about this which is there's no movement at all there's a lexical copying rule such that the word will or the word can these these auxiliary verbs they just have two forms and and and one of them is the declarative and one of them is interrogative and you basically have the declarative one and oh i form the interrogative or i can form one from the other it doesn't matter which direction you go and and i just have a new entry which has the same meaning which has a slightly different argument structure argument structure is just a fancy word for the ordering of the words and so if i say you know it it it was the the dogs two dogs can or will enter the room the the there's two forms of will one is will declarative and and then okay i've got my subject to the left it comes before me and the verb comes after me in that one and then the will interrogative is like oh i go first interrogative will is first and then i have the subject immediately after and then the verb after that and so you just you can just generate from one of those words another word with a slightly different argument structure with different ordering and these are just lexical copies and they're just they're not necessarily moving from one to two movement there's a romantic notion that you have like one main way to use a word and then you could move it around right right which is essentially what movement is implying yeah but that's that's the lexical copying is similar so then you so so then then we we do lexical copying for that same idea that maybe the declarative is the source and then we can copy it and so an advantage for well there's multiple advantages of the lexical copying story it's not my story this is like ivan sog linguists a bunch of linguists have been proposing these stories as well you know in tandem with the movement story okay you know he's a he's ivinsag died a while ago but he was one of the proponents of the non movement for the lexicon copying story and so that is that a great advantage is well chomsky really famously in nineteen seventy one showed that the movement story leads to learnability problems it leads it leads to problems for for how language is learned it's really really hard to figure out what the underlying structure of a language is if you have both phrase structure and movement it's like really hard to figure out what came from what there's like a lot of possibilities there if you don't have that problem learning the learning problem gets a lot easier just say there's lexical copies yeah and yeah yeah when we say the learning problem do you mean like humans learning a new language yeah just learning english so baby is lying around listening to the crib listening to me talk and you know how are they learning english or you know maybe it's a two year old who's learning you know interrogatives and stuff for one you know they're you know how are they doing that are they doing it from like are they figuring out or like no so chomsky said it's impossible to figure it out actually he said it's actually impossible not not hard but impossible mhmm and therefore that's that that's where universal grammar comes from is that it has to be built in and so what they're learning is that there there's some built in movement is built in in his story it's absolutely part of your language module and and then you are you're just setting parameters you're you're set depending on english it's just sort of a variant of the universal grammar and you're figuring out oh which orders do does english do these things that's the the non movement story it doesn't have this it's like much more bottom up you're you're learning rules you're learning rules one by one and oh there's this this word is connected to that word a great another advantage is learnable another advantage of it is that it predicts that not all auxiliaries might move like it it might depend on the word depending on whether you and and and that turns out to be true so there's words that that don't really work as auxiliary you know they work in declarative and not in in interrogative so i can say i'll give you the opposite first if so i can say aren't i invited to the party okay and and that's an that's an interrogative form but it's not from i aren't invited to the party there is no i aren't right so that's decl that's interrogative only and and then we also have forms like ought i i ought to do this and and i guess some british old british people can say i exactly it doesn't sound right does it for me it sounds ridiculous i don't even think ought is great but i mean i totally recognize i ought to do i it's not too bad actually i can say ought to do this that that sounds pretty good yeah if i'm trying to sound sophisticated maybe i don't know it just sounds completely okay i yeah anyway it's it's so there are variants here and a lot of these words just work in one versus the other and and that's like fine under the lexical copying story it's like well you just learn the usage whatever the usage is is what you is what you do with this with with this word but it doesn't it's a little bit harder in the movement story the movement story like that's an advantage i think of lexical copying in all these different places there's there's all these usage variants which make the movement story a little bit harder to work so one of the main divisions here is the movement story versus the last story that has to do about the auxiliary warts and so on but you just rewind to the phrase structure grammar yeah versus dependency grammar those are equivalent in some sense in that for any dependency grammar i can generate a dependency a phrase structure grammar which generates something really salient which is the dependent the the lengths of dependencies between words which isn't so obvious i got it in the in the phrase in the phrase structure it's just kinda hard to see it's in there it's just very very it's opaque technically i think phrase structure grammar is mappable to dependency grammar and vice versa and vice versa yeah yeah there's but there's like these like little labels s and pvp yeah for a particular dependency grammar you can make a phrase structure grammar which generates exactly those same sentences and vice versa but there are many phrase structure grammars which you can't really make a dependency grammar i mean it there you can do a lot more in a phrase structure grammar but you you get many more of these extra nodes basically you you can have more structure in there and and some people like that and and maybe there's value to that i i i don't like it well for you so we we should clarify so so dependency grammar is just well one word depends on only one other word and you form these trees yes and that makes it really puts priority on those dependencies just like as a as a tree that you can then measure the distance of the dependency from one word to the other they can then map to the cognitive processing of the of the sentences how well how easy it is to understand and all that kind of stuff so it just puts the focus on just like the mathematical distance of dependence between words so like it's just a different focus absolutely just continue on the thread of chomsky because it's really interesting because it mhmm as you're discussing disagreement to the degree there's disagreement you're also telling the history of the study of language which is really awesome so you mentioned context free versus regular does that distinction come into play for dependency grammars no okay not at all i mean the regular regular languages are too simple for human languages they they are they it's it's a part of the hierarchy but human languages are in in the phrase structure world are definite they they're at least context free maybe a little bit more a little bit harder than that but so there's something called context sensitive as well mhmm where you can have like this is the just the formal language description in in a context free grammar you have one this is like a bunch of like formal language theory we're doing here but i love it okay so you have you have a left hand side category and you're expanding to anything on the right is is a that's a context free it's like the idea is that that category on the left expands in independent of context to those things whatever they are on the right it doesn't matter what and a context sensitive says okay i i actually have more than one thing on the left i can tell you only in this context you know i have maybe you have like a left and a right context or just a left context or a right context i have two or more stuff on the left tells you how to expand that those things in that way okay so it's context sensitive a a regular language is just more constrained and so it it doesn't allow anything on the right it it allows very it it allow basically it's a one very complicated rule is kind of what a a a regular language is and so it doesn't have any let's just say long distance dependencies it doesn't allow recursion for instance there's no recursion in yeah recursion is where you you which is human languages have recursion they have embedding and you can't well it doesn't allow center embedded recursion which human languages have which is what center embedded recursion so within a sentence within a sentence yeah within a sentence so here we're gonna get to that but i i you know the formal language stuff is a little aside chomsky wasn't proposing it for human languages even he was just pointing out that human languages are context free and then he was most in for for human because that was kind of stuff we did for formal languages and what he was most interested in was human language and that's like the the movement is where we we we where where he sort of set off in on the i would say a a very interesting but wrong foot it was kind of interesting it's a very i agree it's kind of it's a very interesting history so there's this set so he proposed this multiple theories in fifty seven and then sixty five they're they all have this framework though was phrase structure plus movement different versions of the of the phrase structure and the movement in the fifty seven these are the most famous original bits of chomsky's work and then seventy one is when he figured out that those lead to learning problems that that there's cases where a kid could never figure out which rule which set of rules was intended and and so and then he said well that means it's innate it's it's kind of interesting he just really thought the movement was just so obviously true mhmm that he couldn't he he didn't even entertain giving it up it's just obvious that's that's obviously right and it was later where people figured out that there's all these like subtle ways in which things which which look like generalizations aren't generalizations and they you know across the category they're they're word specific and and they have and they they kinda work but they don't work across various other words in the category and so it's easier to just think of these things as lexical copies and and i think he was very obsessed i don't know i'm guessing that he he just he really wanted this story to be simple in some sense and language is a little more complicated in some sense you know he didn't like words he never talks about words he likes to talk about combinations of words and words are you know look up a dictionary there's fifty senses for a common word right the word take will have thirty or forty senses in it so there'll be many different senses for common words and he just doesn't think about that i i it's or doesn't think that's language i think he doesn't think that's language he thinks that words are distinct from combinations of words i think they're the same if you look at my brain on in the scanner while i'm listening to a language i understand and you compare i can localize my language network in a few minutes in like fifteen minutes and what you do is i listen to a language i know i listen to you know maybe some language i don't know or i listen to muffled speech or i i read sentences and i read non words like i do anything like this anything that's sort of really like english and anything that's not very like english so i've got something like it and not and i got a control and and the voxels which is just you know the three d pixels in my in my brain that are responding most are are the is a language area and and that's this left lateralized area in my head and and wherever i look in that network mhmm if you look for the combinations versus the words it's there it's it's everywhere it's the same that's fascinating and and and so it's like hard to find there are no areas that we know i mean that's it's a little overstated right now at this at this point the the technology isn't great it's not bad but we have the best the best way to figure out what's going on in my brain when i'm listening or reading language is to use fmri functional magnetic resonance imaging and that's a very good localization method so i can figure out where exactly these signals are coming from pretty you know down to you know millimeters you know cubic millimeters or smaller okay very small we can figure those out very well the problem is the when okay it's it's measuring oxygen okay and oxygen takes a little while to get to those cells that's what it takes on the order of seconds so i talk fast i i probably listen fast and i can probably understand things really fast so a lot of stuff happens in two seconds and so to say that we know what's going on that the words right now in that network our best guess is that whole network is doing something similar but maybe different parts of that network are doing different things and and that's probably the case we just don't have very good methods to figure that out right at this moment and so since we're kind of talking about the history of the study of language what other interesting disagreements and you're both at mit or were for a long time mhmm what kind of interesting disagreements there tension of ideas are there between you and noam chomsky and we should say that noam was in the linguistics department and you're i guess for a time were affiliated there but primarily brain and cognitive science department which is another way of studying language and you've been talking about fmri so like what is there something else interesting to bring to the surface about the disagreement between the two of you or other people in the in this yeah i i mean i've been at mit for thirty one years since nineteen ninety three and he chomsky's been there much longer so i i met him i knew him i i met when i first got there i guess and i and we would interact every now and then i'd say that so i'd say our our biggest difference is our methods and so i that that's the biggest difference between me and noam is that i gather data from people i do experiments with people and i gather corpus data whatever whatever corpus data is available and we do quantitative methods to evaluate any kind of hypothesis we have he just doesn't do that and so you know you you know he has never once been associated with any experiment or corpus work ever and so it's all thought experiments it's his own intuitions so i i just don't think that's the way to do things mhmm that's a that's a you know across the street they're across the street from us kinda difference between brain and cog sci and linguistics i mean not all some of the linguists depending on what you do more speech oriented they do more quantitative stuff but in the in the meaning words and well it's combinations of words syntax semantics they tend not to do experiments and and corpus analysis so in the linguistic size it probably well the but the method is a symptom of a bigger approach which is sort of a psychology philosophy side on noam and for you it's more sort of data driven so sort of almost like mathematical approach yeah i mean i'm psychologists so i would say we're in psychology you know i'm i'm in brain cognitive sciences is mit's old psychology department it was the psychology department up until nineteen eighty five and it became the brain and cog cognitive science department and so i i mean my training is in psych i mean my training is math computer science but i'm a psychologist i mean i'm i mean i don't know what i am so data driven psychologist yeah yeah yeah you are i know what i am but but i i'm happy to be called a linguist i'm happy to be called a computer scientist i'm happy to be called a psychologist any of those things but in the actual like how that manifests itself outside of the methodology is like these differences these subtle differences about the movement story versus the lexical copy story yeah those are theories right so the theory it's like the theories are i but i think the the reason we differ in part is because of how we evaluate the theories and so i evaluate theories quantitatively and he noam doesn't got it okay well let's let's explore the theories that you explore in your book let's return to this dependency grammar framework of looking at language what's a good why the dependency grammar framework is a good way to explain language what's your intuition so the reason i like dependency grammar as i've said before is that it's very transparent about its representation of distance between words so it's like it if all it is is you've got a bunch of words you're connecting together to make a sentence and a a a really neat insight which turns out to be true is that the further apart the pair of words are that you're connecting the harder it is to do the production the harder it is to do the comprehension it says harder to produce it's hard to understand when the words are far apart when they're close together it's easy to produce and it's easy to comprehend let me give you an example okay so we have in any language we have mostly local connections between words but they're abstract the the connections are abstract they're between categories of words and so you can always make things further apart if you put your if you add modification for example after a noun so a noun in english comes before verb the subject noun comes before verb and then there's an an object after for example so i can say what i said before you know the dog entered the room or something like that so i can modify dog if i say something more about dog after it then what i'm doing is indirect indirectly i'm lengthening the dependence the dependence between dog and entered by adding more stuff to it so i this make just make it explicit here if i say the the boy who the cat scratched cried we're gonna have a mean cat here and and so what i've got here is i get the boy cried it would be a very short simple sentence and i just told you something about the the boy and i told you it was it was the boy who the cat scratched okay so the car ride is connected to the boy boy is technically at the end yep is connected to the boy in the beginning right and so i can do that i can say that that's a perfectly fine english sentence and i can say the cat which the dog chased ran away or something okay i can do that but i it's really and and so i but it's really hard now i i've got you know whatever i have here i have the boy who the cat now let's say i try to modify cat k the boy who the cat which the dog chased scratched ran away oh my god that's hard right i i can i'm sort of just working that through in my head how to produce and how to and and it's really very just just horrendous to understand it's not so bad at least i've got intonation there to sort of mark the boundaries and stuff but it's that's really complicated that's sort of english in a way i mean that follows the rules of english but so what what's interesting about that is is that what i'm doing is nesting dependencies here i'm putting one i've got a subject connected to a verb there and and and then i'm modifying that with a a a clause another clause which happens to have a subject and a verbal relation i'm trying to do that again on the second one and what that does is it lengthens out the dependence multiple dependence actually get get lengthened out there the dependencies get get longer longer on the outside ones get long and even the ones in between get kinda long and and and you just so what's fascinating is that that's bad that's really horrendous in english mhmm but that's horrendous in any language and so in in no matter what language you look at if you do just figure out some structure where i'm gonna have some modification following some head which is connected to some later head and i do it again it won't be good guaranteed like one hundred percent that will be uninterpretable in that language in the same way that was uninterpretable in english just to clarify the distance of the dependencies is whenever the boy cried this there's a dependence between two words and then you're counting the number of what morphemes between them that's a good question i i just say words your words are morphemes between we don't we don't know that actually that's a very good question what is the distance metric but let's just say it's words sure k so the and you're saying the longer the distance of that dependence the more no matter the language except legalese legalese even legalese okay we'll talk we'll we'll talk about it okay okay okay but that the people will be very upset that speak that language not upset but they'll either not understand it or they'll be like this is they'll their brain will be working in overtime yeah they will have a hard time either producing or comprehending it they might tell you that's not their language you know it's sort of the language i mean it it's following their like they'll agree with each of those pieces as part of their language but somehow that combination will be very very difficult to produce and understand is that a chicken or the egg issue here so like is well i'm giving you an explanation right so the egg i mean well i mean and then there's i'm giving you two kinds of explanations i'm telling you that center embedding that's nesting those are the same those are synonyms for the same concept here and i'm the explanation for what those are always hard center bedding and nesting are always hard and i gave you an explanation for why they might be hard which is long distance connections there's a a when you do central embedding when you do nesting you always have long distance connections between the dependents you just and so that's not necessarily the right explanation it just hap i i can go through reasons why that's probably a good explanation and it's not really just about one of them it so probably it's a a pair of them or something of these dependents that you get get long that drives you to like be really confused in that case and so what the the behavioral consequence there if you i mean we this is kind of methods like how do we get at this you could try to do experiments to get people to produce these things they're gonna have a hard time producing them you can try to do experiments to get them understand them when you get and then you see how well they understand them mhmm can they understand them another method you can do is give people partial materials and ask them to complete them you you know those those center embedded materials and and they they'll fail so i've done that i've done all these kinds of things so so so what what what do you mean so for for the so central bedding meaning like you take a normal sentence like boy cried and inject a bunch of crap in the middle yes that separates the boy and the cried yes okay that's central bedding and nesting is same thing on top of that no no nesting is the same thing central bedding those are totally equivalent terms i'm sorry i sometimes use one and sometimes you use got it got it they don't anything different got it and then what you're saying is there's a bunch of different kinds of experiments you can do i mean i like the understanding one is like have more embedding more central embedding is it easier or harder to understand but then you have to measure the level of understanding i guess yeah yeah you could i mean there's multiple ways to do that i mean there's there's the simplest way is just ask people how good does it sound how natural does it sound that's a very blunt but very good measure it's very very reliable people will do the same thing and so it's like i don't know what it means exactly but it's doing something such that we're measuring something about the confusion the difficulty associated with those and those like those are giving you a signal that's why you can say them k yeah very good what about the the completion of this with the central bank so if you give them a partial sentence say i say the book which the author who and i ask you to now finish that off for me i mean either say it yeah yeah but you can just but if say it's written in front of you and you can just type and have much print time as you want they will even though that one's not too hard right so if i say it's like the book it's like oh the the book which the author who i met wrote was good you know that's a very simple completion for that you know if i give that to completion on online somewhere to a you know a crowdsourcing platform and ask people to complete that they will miss off of a verb very regularly like half the time maybe two thirds of the time they'll say they'll just leave off one of those verb phrases even with that simple so say the book which the author who and they'll say was they won't you need three verbs right i need three verbs here who i met wrote mhmm was good and they'll give me two they'll say who was who was famous was good or something like that they'll just give me two and and and that they'll happen about sixty percent of the time so forty percent maybe thirty they'll do it correctly incorrectly meaning they'll do a three verb phrase i don't know what's correct or not you know this is hard it's a hard task yeah i can actually i'm struggling with it in my head yeah well it's it's easier written when you when you stare at it if you look it's a little easier than listening is pretty tough because you have to because there's no trace of it you have to remember the words that i'm saying which is very hard auditorily we wouldn't do it this way we you'd do it written you can look at it and figure it out it's easier in many dimensions in some ways depending on the the person it's easier to gather written data for i mean most sort of psych i work in psycholinguistics right psychology of language and stuff and so a lot of our work is based on written stuff because it's so easy to gather data from people doing written kinds of tasks spoken tasks are just more complicated to administer and analyze because people do weird things when they speak and it's harder to analyze what they do but they they they they generally point to the same kinds of things so okay so the the universal theory of language yeah by ted gibson is that you can form dependency you can form trees from any sentences and that's right you can measure the distance in some way of those dependencies and then you can say that most languages have very short dependencies all languages all languages all languages have short dependencies you can actually measure that so i i an ex student of mine these guys at university of california irvine richard futrell did a thing a bunch of years ago now where he looked at all the languages we could look at which was about forty initially and now i think there's about sixty for which there are dependency structures like you so they're meaning there's gonna be like a big text bunch of texts which have been parsed for their dependency structures and there's about sixty of those which have been parsed that way and for all of those you can what what he did was take any any sentence in one of those languages and and you can do the dependency structure and then start at the root we were we're talking about fancy structures that's pretty easy now and and he's trying to figure out what a control way you might say the same sentence is in that language and so we just just like alright there's a root and it had let's say as a sentence is let's go back to you know two dogs entered the room so entered is the root and entered has two dependents it's got dogs and it has room okay and what he did is like let's scramble that order that's three things the root and the the the head and the two dependents and into some random order mhmm just random and then just do that for all the dependents down the two for so now look do it for the and whatever was two in dogs and for and room and and that's you know that's not a very short sentence when sentences get longer and you have more dependence there's more scrambling that's possible and what he found was so that so so that that's one you could figure out one scrambling for that sentence it is like hundred times for every sentence in every corp in every one of these texts every corpus and and then he just compared the dependency lengths in those random scramblings to what actually happened what what the the english or the french or the german was in the original language or chinese or what all these eighty lang no sixty languages okay and and the dependency lengths are always shorter in the real language compared to the to this this kind of a control and there's another like he it's a it's a little more rigid his control so the the way i described it you could have crossed dependencies like the mhmm by by scrambling that way you could scramble in any way at all languages don't do that they tend not to cross dependencies very much like so the dependency structure they just they tend to keep things noncrossed and there's a you know like there's a technical term they call that projective but it's just noncrossed is all that is projective and so if you just constrain the the scrambling so that it only gives you projective sort of non noncrossed it's the same thing holds so it's so the you still still human languages are much shorter than these this kind of a control so there's like what it means is that that that we're in every language we're trying to put things close in in relative to this kind of a control like there it doesn't matter about the word order some of these are verb final some of them use a verb medial like english and some are even verb initial there are a few languages of the world which have vso world or word order verb subject object languages haven't talked about those it's like ten percent yeah of the and even even in those languages it's still short dependencies short dependencies is rules okay so how what what what are some possible explanations for that the for why why languages have evolved that way so that that's one of the i suppose disagreements you might have with chomsky so you consider the evolution of language in in terms of information theory yeah and for you the purpose of language is ease of communication right and process that's right that's right so i mean the the story here is just about communication it is just about production really it's about ease of production is the story when you say production can you can you oh i just mean each of language production it's easier for me to say things yeah when the come i what i'm doing whenever i'm talking to you is somehow i'm formulating some idea in my head and i'm putting these words together yeah and it's easier for me to do that to put to say something where the words are close closely connected in a dependency as opposed to separated like by putting something in between and over and over again i it's just hard for me to keep that in my head it like that's that's the whole story like the story it's basically i say that the dependency grammar sort of gives that to you like just like long long is bad short is is good it's like easy to keep in mind because you have to keep it in mind for probably for production you know probably matters in comprehension as well like also matters in comprehension so on both sides of it yeah the production and the but i would guess it's probably evolved for production it's about producing it's what's easier for me to say that ends up being easier for you also and and that's very hard to disentangle this idea of who is it for is it for me the speaker or is it for you the listener i mean part of my language is for you like the way i talk to you is gonna be different from how i talk to different people i'm i'm definitely angling what i'm saying to who i'm saying right it's not like i'm just talking the same way to every single person and so i am sensitive to my audience but how does that does that you know work itself out in the in the dependency link audience yeah but but it's both it's just kinda like messing with my head a little bit to say that some of the optimization might be or maybe the primary objective of the optimization might be the ease of production yeah so we have different senses i guess i'm i'm like very selfish and you're i'm like i think it's like it's all about me i'm like i'm just doing what's easiest for me it all sucks i don't wanna i'm like i'll i mean but i have to of course choose the words that i think you're gonna know i'm not gonna choose words you don't know in fact i'm gonna fix that when i you know so there it's about but but maybe for for the syntax for the combinations it's just about me i feel like it's i i don't know though it's very wait wait wait wait but the purpose of communication is to be understood is to convince others and so on yeah so like the selfish thing is to be understood okay yeah it's it's about a little circular there too then okay right i mean like the ease of production help that that helps me be understood then i don't think it's circular so i want what's the primary i i think the primary objective is to be under is about the listener because otherwise the if you're optimizing to for the ease of production then you're you did you're not gonna have any of the interesting complexity of language like you're trying to like explain control for what it is i wanna say like i i'm saying let's control for the thing the the message control for the message but that means the message needs to be understood that's the goal but that's the meaning so i'm still talking about the form the form just the form of the meaning how do i frame the form of the meaning is all i'm talking about you're talking about a harder thing i think it's like how am i like try to change the let's con let's keep the meaning constant like which you got it we have you keep the meaning constant how can i phrase whatever it is i need to say like i gotta pick the right words and i'm gonna pick the order so that it's so it's easy for me you know that's that's that's that's what i think is probably like i think i'm still tying meaning and form yeah together in my head mhmm but you're saying if you keep the meaning of what you're saying constant yeah would the optimization yeah it could be the primary objective of that optimization is the for production that's interesting i'm i'm struggling to keep constant the meaning it's just so i mean i'm i'm such a i'm a human right so for me the form without having introspected on this the form and the meaning are tied together like deeply because i'm a human like for for me when i'm speaking because i haven't thought about language mhmm mhmm like in a rigorous way about the form of language look for any event there's there's an you know unbounded i don't i don't wanna say infinite but sort of you know ways of that i might communicate that same event this two dogs entered a room i can say in many many different ways i can say hey there's two dogs they entered the room hey the room was entered by something the thing that was entered was two dogs i mean there's i mean it's kind of awkward and weird and stuff but those are all similar messages mhmm with different forms but different ways i might frame and of course i use the same words there all the time i could have referred to the dogs as you know a dalmatian and a poodle or something you know i there i could have been more specific or less specific about what they are and i could have said been more abstract about about about the number there's like so i like i'm trying to keep the meaning which is this event constant and then how am i gonna describe that to get that to you it kinda depends on what you need to know right and what i think you need to know but i'm like trying to let let's control for all that stuff and not and and then like i'm just like choosing about i'm doing something simpler than you're doing which is just forms yes just words to use specifying the specie the the breed of dog and whether they're cute or not is changing the meaning that might be yeah yeah that would be change oh that would be changing the meaning for sure right so you're just yeah yeah well yep yeah that's changing the meaning but say even if we keep that constant we can still talk about what's easier or hard for me right the listener and the and the right i can know which phrase structures i use which combinations which you know this is so fascinating and it's just like a a really powerful window into human language but i wonder still throughout this how vast the gap between meaning and form i just i just have this like maybe romanticized notion that they're close together that they evolve close like hand in hand that you can't just simply optimize for one without the other being in the room with us like it's well it's kinda like an iceberg form is the tip of the iceberg and the rest the the meaning is the iceberg but you can't like sep but i think that's why these large language models are so successful is because they they're good at form and form isn't that hard in some sense and meaning is tough still and that's why they're not they're you know they don't understand what they're we're gonna talk about that later maybe but like we can distinguish in our forget about large language models like humans we're maybe you'll talk about that later too it's like the difference between language which is a communication system and thinking which is meaning so language is a communication system for the meaning it's not the meaning and so that's why i mean that and there's a lot of interesting evidence we can talk about relevant to that well i mean that that's a really interesting question what is the different what is the difference between language written communicated versus thought what to use the difference between them well you or anyone has to think of a task which they think is is a good thinking task mhmm and there's lots and lots of tasks which should be good thinking tasks and whatever those tasks are let's say it's you know playing chess or that's a good thinking task or playing some game or doing some complex puzzles mhmm maybe maybe remembering some digits that's thinking remember some a lot of different tasks we might think maybe just listening to music is thinking or there's a lot of different tasks we might think of as thinking there's this woman in my department she's done a lot of work at on on this question about what's the connection between language and thought mhmm and and so she uses i was referring earlier to mri fmri that's her primary primary method and so she is been really fascinated by this question about whether what languages okay and so as i mentioned earlier you can localize my language area your language area in a few minutes okay like fifteen minutes i can listen to language listen to non language or backward speech or something and and and we'll find areas left lateralized network in my head which is especially which is very sensitive to language as opposed to whatever that control was okay can't specify what you mean by language like communicating language like what is lang just sentences you know i'm listening to english of any kind story or i can read sentences anything at all that i understand if i understand it then it'll activate my language network so right now my language network is going like crazy when i'm talking and when i'm listening to you because we're both we're communicating and that's pretty stable yeah it's incredibly stable so i've i i happen to be married to this woman federico and so i've been scanned by her over and over and over since two thousand and seven or six or something and so my language network is exactly the same you know like a month ago as it was back in two thousand seven it's amazingly stable it's astounding and with it it's it's a really fundamentally cool thing and so my language network is it's like my face okay it's not changing much over time inside my head can i ask a quick question sorry it's a small tangent at which point in the as you grow up from baby to adult does it stabilize we don't know like that's a that's a very hard question they're working on that right now because of the problem scanning little kids like doing the trying to do local trying to do the the localization on little children in in this scanner you're lying in the fmri scan that's the best way to figure out where something's going on inside our brains and the scanner is loud and you're in this tiny little area you're claustrophobic and it doesn't bother me at all i can go to sleep in there but some people are bothered by it the little kids don't really like it and they don't like to lie still and you have to be really still because if you move around that's that messes up the coordinates of where where everything is and so you know try to get you know your question is how and when are language developing you know how it when did how does this left lateralized system come to play where does it you know and it's really hard to get a two year old to do this task but you can maybe where they're starting to get three and four and five year olds to do this task for short periods and it looks like it's there pretty early so clearly when you lead up to like a baby's first words before that there's a lot of fascinating turmoil going on about like figuring out like what are what are these people saying yep and you're trying to like make sense how does that connect to the world and all that kind of stuff yeah there that that might be just fascinating development that's happening there that's yeah hard to introspect but anyway you wait we're back to the scanner yes and i can find my network in fifteen minutes and now we can ask a we can ask find my network find yours find you know twenty other people do this task yep and and we can do some other tasks anything else you think is thinking of some other thing i can do a spatial memory task i can do a music perception task i can do programming task if i program okay i can do i where i can like understand computer programs mhmm and none of those tasks will tap the language network at all like at all there's no overlap they do they're they're highly activated in other parts of the brain there's a there's a bilateral network which i think she she tends to call the multiple demands network which does anything kinda hard and and so anything that's kind of difficult in some ways will activate that multiple demands network i mean music will be in some music area you know there's music specific kinds of areas and so but there but but none of them are activating the language area at all unless there's words like so if you have music and there's a song and you can hear the words then then then you get the language area we're talking about speaking and listening but are or are we also talking about reading this is all comprehension of any kind and so so that is fascinating so what this this this network doesn't make any difference if it's written or spoken so the the lang the thing that she calls federico calls the the language network it's this high level language so it's not about the spoken the spoken language and it's not about the written language it's about either one of them and so we're so when you do speech you're sort of list you're you either you listen to speech and you and you subtract away some language you don't understand and so which and or you subtract away back backwards speech which signs sounds like speech but isn't and and then so you you take away the sound part altogether and so and then if you do written you get exactly the same network so for just reading the language versus reading sort of nonsense words or something like that you you'll find exactly the same network and so this is about high level the comprehension comprehension of language yeah in this case and and the same thing happen production's a little harder to run the scanner but the same thing happens in production you get the same network so production's a little harder right you have to figure out how do you run a task you know in the network such that you're doing some kind of production and i can't remember what they've done a bunch of different kinds of tasks there where you get people to control produce things yeah figure out how to produce and the same network goes on there exactly the same place so if you wait wait so if you read random words yeah if you read things like like gibberish yeah yeah lewis carroll's twas brillig jabberwocky right they call that jabberwocky speech or the the network doesn't get activated not as much there are words in there yeah because it's still there's there's function words and stuff so it's lower activation fascinating yeah yeah so there's like basically the more language like it is my right the higher it goes in the language network and that network is there from when you speak from as soon as you learn language and and and it's it's there like you you speak multiple languages the same network is going for your multiple languages so you speak english you speak russian then the both of them are hitting that same network if you're if you're fluent in those languages so programming not at all isn't that amazing even if you're a really good programmer that is not a human language it's just not conveying the same information and so it is not in the language network and so that is mind blowing as i think that's weird that's weird it's amazing that's weird it's so that's like one set of data this is hers like shows that what you might think is thinking is is not language language is just the seek just just this conventionalized system that we've worked out in in human languages oh another fascinating little bit tidbit is that even if they're these constructed languages like klingon or i don't know the languages from game of thrones i'm sorry i don't remember those languages but you there's a lot of people offended right now there's people that speak those languages yes they they they really speak those languages because the people that wrote the languages for the shows they did an amazing job of constructing something like a human language and those that that lights up the language area that's like because they can speak you know pretty much arbitrary thoughts in a human language it's not a it's a constructed human language probably it's related to human languages because the people that were constructing them was were making them like human languages in various ways but it it also activates the same network which is pretty really cool anyway sorry to go into a place where you may be a little bit philosophical but is it possible that this area of the brain is doing some kind of translation into a deeper set of it's almost like concepts which it has to be doing so it's doing in communication right it is translating from thought whatever that is it's more abstract and it's doing that that's what it's doing like it is that that is kinda what it is doing it's like kind of a meaning network i guess yeah like a translation network yeah but i wonder what is at the core at the bottom of it like what are thoughts are they are thoughts to me like i don't know thoughts and words are they neighbors or are are is it one turtle sitting on top of the other meaning like is there a deep set of concepts that we well there's connections right between this what what these things mean and then there's probably other other parts of the brain that what these things mean and so you know when i'm talking about whatever it is i wanna talk about if it's some it it'll be represented somewhere else that that knowledge of whatever that is will be represented somewhere else well i wonder if there's like some stable mean nicely compressed encoding of meanings i don't know that's separate from language the link you know i guess i guess the implication here is that that we don't think in language that's correct isn't that cool and and and that's so interesting so people i mean i this is like hard to do experiments on but yeah there is this idea of in your inner voice and a lot of people have an inner voice and so if you do a poll on the internet and ask if you you hear yourself talking when you're just thinking or whatever you know about seventy or eighty percent of people will say yes most people have an inner voice i i don't and so i always find this strange when so when people talk about an inner voice i always thought this was a metaphor and and they hear i'm i know most of you whoever's listening to this thinks i'm crazy now because i'm i don't have an inner voice and i i just don't know what you're listening to i i just it sounds so kind of annoying to me but to to have this voice going on while you're while you're thinking but i guess most people have that and i don't have that and i don't we don't really know what that that connects to i wonder if the inner voice activates that same network i don't i wonder i don't i don't know i don't know but i mean this could be speechy right so that's like the the you hear do you have an inner voice i don't think so oh a lot of people have the sense that they hear other pea they hear themselves and then say they read someone's email i've heard people tell me that they hear that other person's voice when they read other people's in emails and i'm like wow that sounds so disruptive i do think i like vocalize what i'm reading but i don't think i hear a voice mhmm well that's you probably don't have an inner voice yeah i don't think i have an inner voice have an inner voice people have this strong percept of hearing sound in their heads when they're just thinking i refuse to believe that's the majority of people majority absolutely what it's it's like two thirds or three quarters it's not i i whenever i ask class they and and i went internet they always say that so you're you're in a minority it could be a self report flaw it could be you know when i'm reading yeah inside my head i'm kind of like saying the words which is probably the wrong way to read but i don't hear a voice there's no press precept of a voice i refuse to believe the majority of people have it anyway they do it's a fascinating the human brain is fascinating but it still blew my mind that the that language does appear comprehension does appear to be separate from thinking mhmm so that's one set one set of data from fedorenko's group is that no matter what task you do if it doesn't have words and combinations of words in it then it won't light up the language network and you know you could it'll be active somewhere else but not there so that's one and then this other piece of evidence relevant to that question is there it turns out there are these this group of people who've had a massive stroke on the left side and wiped out their language network and it as long as they didn't wipe out everything on the right as well in that case they wouldn't be you know cognitively functionable but if they just wiped out language which is pretty tough to do because it's it's very expensive on the left mhmm but if they have then there are these there is patients like this called so called global aphasia who can do any task just fine but not language they can't they can't talk to them i mean you they don't understand you they can't speak they can't write they can't read but they can do all they can play chess they can drive their cars they can do all kinds of other stuff you know do math they can do all like so math is not in the language area for instance you do arithmetic and stuff that's not language area it's got symbols so people sort of confuse some kind of symbolic processing with language and symbolic processing is not the same so there are symbols and they have meaning but it's not language it's not a you know conventionalized language system and so lang so math isn't there and so they can do math they're they do just as well as their control age match controls and all these tasks this is rosemary varley over in university college london who has a bunch of patients who who she's shown this that they're just so that that sort of combination suggests that language isn't necessary for thinking it it it doesn't mean you can't think in language you could think in language because language allows a lot of expression but it's just you don't need it for thinking it's it suggests that language is separate is a separate system from this is kinda blowing my mind right now it's cool isn't it trying to load that in yeah yeah because that has implications for large language models it sure does and they've been working on that well let's take a stroll there you wrote that the best current theories of human language are arguably large language models so this has to do with form it's a kind of a big theory and but the reason it's arguably the best is that it it does the best at predicting what's english for instance it's it's like incredibly good you know better than any other theory it's so you know but you know we don't you know there's it's not sort of there's not enough detail well it's opaque like there's not you don't know what's going on you don't know what's going on it's it's another black box but i think it's you know it is a theory what's your definition of a theory because it's a gigantic it's a gigantic black box with a you know a a very large number of parameters controlling it to me theory usually requires a simplicity right well i don't know maybe i'm just being loose there i i think it's a it's not a it's not a great theory but it's a theory it's a good theory in in one sense and that it covers all the data like anything you wanna say in english it does and so that's why it's that's how it's arguably the best is that no other theory is as good as a large language model in in predicting exactly what's good and what's bad in english you know you now you're saying is it a good theory well probably not you know because i i want a smaller theory than that it's too big i agree you could probably construct mechanism by which it can generate a simple explanation of a particular language like a set of rules something like a it could could generate a a dependency grammar for a language right yes you could probably mhmm you could probably just ask it about itself you know that's i mean that presumes and and there's some evidence for this that that that some large language models are implementing something like dependency grammar inside them and so there's work from a guy called chris manning and colleagues over at stanford mhmm in in natural language and they looked at i don't know how many large language model types but certainly bert and some others where and and where where you do some kind of fancy math to figure out exactly what the sort of what what kind of abstractions of representations are going on and they and they were saying it does look like dependency structure is is what they're constructing it doesn't like so it's actually a very very good map so kind of a they are constructing something like that does it mean that you know that they're using that for meaning i mean probably but we don't know you're right that the kinds of theories of language that llms are closest to are called construction based theories can you explain what construction based theories are it's just a general theory of language such that there's a form and a meaning pair for for lots of pieces of the language and so it's it's it's primarily usage based is the construction grammar it's just it's trying to deal with the things that people actually say actually say and actually write and so that's it's a usage based idea and what's a construction a construction is either a simple word so if of like a a morpheme plus its meaning or a combination of words it's basically combinations of words like the the rules so but it's it's it's unspecified as to what the form of the grammar is under underlyingly and so i i would i i would argue that the dependency grammar is maybe the the right form to use for the types of construction grammar construction grammar typically isn't kind of formalized quite and so maybe the formalization a formalization of that it might be in dependency grammar i mean i i would think so but i mean it it's up to people other researchers in that area if they agree or not so do you think that large language models understand language are they mimicking language i guess the deeper question there is are they just understanding the surface form or do they understand something deeper about the meaning that then generates the form i mean i would argue they're doing the form they're doing the form they're doing it really really well and are they doing the meaning no probably not i mean there's lots of these examples from various groups showing that they can be tricked in in all kinds of ways they really don't understand the the meaning of what's going on and so there's a lot of examples that he and other groups have given which just which show they don't really understand what's going on so you you know the monty hall problem is this silly problem right where you know if if you have three door that it's it's let's make a deal is this old game show and there's three doors and there's a prize behind one and there's some junk prizes behind the other two and you're trying to select one and if you you know he knows monty he knows where the target item is the good thing he knows everything is one of the doors and it's some junk prize and then the question is should and then the question is should you trade to get the other one and and the answer is yes you should trade because he knew which ones you could turn around mhmm and so now the odds are two thirds okay and then you just change that a little bit to the large language model the large and language model has seen that that that explanation so many times that it just if you change the stories a little bit but it make it sound like it's the monty hall problem but it's not mhmm you just say oh there's three doors and one behind them is is a good prize there's two bad doors i happen to know it's behind door number one the good prize the car is behind door number one so i'm gonna choose door number one monty hall opens door number three and shows me nothing there should i trade for door number two even though i know the good prize in door number one and then and then the large language model say yes you should trade because it's a it's it just goes through the the the the the forms that it's seen before so many times on these cases where it's yes you should trade because you know your odds have shifted from one in three now to two out of three to being that thing it doesn't have any way to remember that actually you have a hundred percent probability behind that door number one you know that that's not part of the of the the scheme that it's seen hundreds and hundreds of times before and so you can't you can't even if you try to explain to it that it's wrong that they can't do that it'll just keep giving you back the the the problem but it's also possible the larger language model would be aware of the fact that there's sometimes overrepresentation of of a particular kind of formulation mhmm and it's easy to get tricked by that and so you could see if they get larger and larger models be a little bit more skeptical so you see overrepresentation so like you it just feels like form can generate things that look like the thing understands deeply the underlying world world model mhmm of the kind of mathematical world physical world psychological world that would generate these kinds of sentences it just feels like you're creeping close to the meaning part easily fooled all this kind of stuff but that's humans too so it just seems really impressive how often it seems like it understands concepts i i mean you don't have to convince me of that i'm i am very very impressed but does it does do i mean you're you're giving a possible world where maybe someone's gonna train some other versions such that it'll be somehow abstracting away from types of forms i i mean i don't think that's happened and so well no no no i i'm i'm not saying that i i i think when you just look at anecdotal examples mhmm and just showing a large number of them where it doesn't seem to understand yeah and it's easily fooled yes that does not seem like a scientific well he data driven like analysis of like how many places is damn impressive oh oh no in terms of meaning and understanding and how many places is easily fooled and like that's not the inference yeah so i don't wanna make that the inference i don't i wouldn't wanna make was that in the inference i'm trying to push is just that is it is it like humans here it's probably not like humans here it's different oh it's different don't make that error if you explain that to them they're not gonna make that error you you know they don't make that error and so that's something it's doing something different from humans are that they're doing in that case what what what's the mechanism by which humans figure out that it's an error i'm just saying the error there is like if i explain to you there's a hundred percent chance that the car is behind this case the the this door will you do you wanna trade people say no but this thing will say yes because it's so it's that that trick it's so wound up on the form that it's that that's an error that a human doesn't make which is kind of interesting less likely to make i should say yeah less likely less likely because like humans are very oh yeah i mean you're asking you know you're asking humans to you're asking a system to understand a hundred percent like asking some mathematical concepts and so like look the places where large language models are the form is amazing so let let's go back to yes nested structure center embedded structures okay if you ask a human to complete those they can't do it neither can a large language model they're just like humans in that if you ask if i ask a large language model fascinating by the way that that's the central embedding yeah the central embedding is is is struggles with just like human exactly like humans exactly the same way as humans they get and that's not trained so they do exactly so there so that is the similarity so but then it's it's that's not meaning right this is form but when we get into meaning this is where they get kind of messed up when you start just saying oh what's behind this door oh it's you know it's the thing i want humans don't mess that up as much you know here they they the form is it's just like the form of the match is amazing similar without being trained to do that i mean it's trained in the sense that it's getting lots of data which is just like human data but it's not being trained on you know bad sentences and being told what's bad it just can't do those it'll actually say things like those are too hard for me to complete or something which is kind of interesting actually kind of how does it know that i don't know but it really often doesn't just complete send they get off very often says stuff that's true mhmm and sometimes says stuff that's not true and almost always the form is great yeah but it's still very surprising that with really great form it's able to generate a lot of things that are true based on what it's trained on and so on yes yes so it's not just it's not just form that it's generating it's mimicking true statements that's right that's right i know the internet i guess i guess the underlying idea there is that on the internet truth is overrepresented yeah versus falsehoods i think that's probably right yeah so but the the the fundamental thing is trained on you're saying is just form it's i think so yeah yeah i think so well that's a sad if that's to me that's still a little bit of open question i probably lean agreeing with you especially now you've just blown my mind that there's a separate module in the brain for language versus thinking maybe there's a fundamental part missing from the large language model approach that lacks the thinking the reasoning capability yeah that's what this group argues so the the same group fedorenko's group has a recent paper arguing exactly that there's a guy called kyle mahwell who's here in austin texas actually he's an old student of mine but he's a faculty in linguistics at texas and he was the first author on that that's fascinating still to me an open question yeah what year are the interesting limits of llms you know i i don't see any limits to their form their form is impressive yeah yeah yeah it's pretty i mean it's close to well you said ability to complete central embeddings yeah it's just the same as humans it seems the same as humans but that's not perfect right it should be able that's good no but i want to be like humans i i'm trying i want a model of humans but but but you oh wait wait wait wait oh so perfect to you is is as close to humans as possible i got it yeah but you you should be able to if you're not human you're like you're superhuman you should be able to complete central embedded senses right i mean the that's the the mechanism is if it's modeling some i think it's kind of really interesting that it can really interesting that it's more like like i think it's potentially underlyingly modeling something like what the the way the form is processed the form of human language yeah the way that how and how humans process the language yes i think that's plausible and how they generate language process language and general language that's fascinating yeah so in that sense they're perfect if we can just linger on the center embedding thing that's hard for our allows to produce and that seems really impressive because that's hard for humans to produce and how does that connect to the thing we've been talking about before which is the dependency grammar framework in which you view language and the finding that short dependencies seem to be a universal part of language so why is it hard to complete center embeddings so what i like about dependency grammar is it makes the cognitive cost associated with long longer distance connections very transparent basically there's there's some there it turns out there is a cost associated with producing and comprehending connections between words which are just not beside each other the further apart they are the worse it is the the according to well we can measure that and there is a cost associated with that can you just linger on what do you mean by cognitive cost and how do you measure it oh well you can measure it in a lot of ways the simplest is just asking people to say whether you know how good a sentence sounds which is ask that's one way to measure and you you try to like triangulate then across sentences and across structures to try to figure out what the source of that is you can look at reading times in controlled materials you know and so in certain kinds of materials when the and then we can like measure the dependency distances there we can there's a recent study which looked at we're we're talking about the the brain here we could look at the language network okay we can look at the language network and we could look at the activation in the language network and how big the activation is depending on the length of the defenses and it turns out in just random sentences that you're listening to if you're listening to so it turns out there are people listening to stories here and the bigger the longer the dependence dependency is the the the stronger the activation in the language in the language network and so there's some measure there's a different there's a bunch of different measures we could do that's a kind of a neat measure actually of actual activations activation in the brain so that you can somehow in different ways convert it to a number i wonder if there's a beautiful equation connecting cognitive costs and length of dependency mhmm e equals m c squared kinda thing yeah it's it's complicated but probably it's doable i would yeah i would guess it's doable i you know i tried to do that a while ago and i was reasonably successful but some for some reason i stopped working on that i do i agree with you that it would be nice to figure out so there's like some way to figure out the the the cost i mean it's it's complicated another issue you raised before was like how do you measure distance is it words is it probably isn't is the part of the problem is that some words matter them more than others and probably you know meaning like nouns might matter depending and then it maybe depends on which kind of noun is it a noun we've already introduced or a noun that's already been mentioned is it a pronoun versus a name like like all these things probably matter so probably the simplest thing to do is just like oh let's forget about all that and just think about words or morphemes and for sure but there might be a like there might be some insight in the kind of function yeah yeah that fits the data meaning like a quadratic like what i i think it's an exponential ex exponential we we think it's probably an exponential such that the longer the distance the less it matters and so then then it's the sum of those is my that that was our best guess a while ago so that you've got a bunch of dependencies if you've got a bunch of them that are being connected at some point that's at at the ends of those the the cost is the is some exponential function of those is my guess but because the the reason it's probably an exponential is like it's it's not just the distance between two words because i can make a very very long subject verb dependency by adding lots and lots of noun phrases and prepositional phrases and it doesn't matter too much it's when you do nested when i have multiple of these then then things get go really bad go south probably somehow connected to working memory or something like this that's probably the a function of the memory here is yeah is the access is trying to find those earlier things it's kinda hard to figure out what was referred to earlier those are those connections that's that's the sort of notion of as opposed to a storgy thing but trying to connect retrieve those earlier words depending on what was in between and then then we're talking about interference of similar things in between that's the right theory probably has that kind of notion in it there's an interference of similar and so i i'm i'm dealing with an abstraction over the right theory which is just you know let's count words it's not right but it's close and then may maybe you're right though there's some sort of an exponential or something on on the on the to figure out the total so we can figure out a function for any given for every any given sentence in any given language but you know it's funny you know people haven't done that too much which i do think is i i'm interested that you find that interesting i really find that interesting and i'm a lot of people haven't found it interesting and i don't know why i haven't got people to wanna work on that i really like that too no that's a that's a that's a beautiful and the underlying idea is beautiful that there's a cognitive cost that correlates with the length of dependency mhmm it just it feels like it's a deep i mean language is so fundamental to the human experience and it this is a nice clean mhmm theory of language where yeah it's like wow okay so like we like our words close together yeah they depend on words close together yeah that's why i like it too it's so simple yeah the simplicity of the theory is now and yet it explains some very complicated phenomena if you if i write these very complicated sentences it's kinda hard to know why they're so hard and you can like oh nail it down i can do like i give you a math formula for why each one of them is bad and where and that's kinda cool i think that's very neat have you gone through the process is there like a if you take a piece of text and then simplify sort of like there's an average length of dependency and then you like you know reduce it and see comprehension on the entire not just a single sentence but like you know you go from james joyce to hemingway or something it didn't no no simple answer is no that does there's probably things you can do in that in that kind of direction that's fun we might you know we we we're gonna talk about legalese at some point yes and so we maybe we'll talk about that kind of thinking with with with applied to legalese but let's talk about legalese because you mentioned that as an exception we're just taking tangent upon tangent that's an interesting one you're given as an exception it's an exception that you say that most natural languages as we've been talking about have local dependencies with one exception legalese that's right so what is legalese first of all oh well legalese is what you think it is it's just any legal language i mean like i actually know know very little about the kind of the laws use mhmm so i'm just talking about language in laws and language in contracts got it so the stuff that you have to run into we have to run into every other day or every day and you skip over because it reads poorly and or you know partly it's just long right there's a lot of text there that we don't really wanna know about and so but the the thing i'm interested in so i i've been working with this guy called eric martinez who is a he was a lawyer who was taking my class i i was teaching a psycholinguistics lab class and i have been teaching it for a long time at mit and he's a he was a law student at harvard and he took the class because he had done some linguistics as an undergrad and he was interested in the problem of why legalese sounds hard to understand you know why and and so why is it hard to understand and why do they write that way if it is so hard to understand it seems apparent that it's hard to understand the question is why is it and so we we didn't know and we did an evaluation of bunch of contracts actually we just took a bunch of sort of random contracts because i don't know you know there's contracts and laws might not be exactly the same but contracts are kind of the things that most people have to deal with most of the time and so that's kind of the most common thing that humans have like humans that that adults in our industrialized society have to deal with a lot and so so that's what we we pulled and we we didn't know what was hard about them but it turns out that the way they're written is is very center embedded has nested structures in them so it has low frequency words as well that's not surprising lots of texts have low it does have surprising slightly lower frequency words than other kinds of control text even sort of academic text legal is even worse it is the worst that that we were being assigned you just you just revealed again that lawyers are playing they're they're optimizing a different well you know it's interesting that's a that's a now you're getting at why and so and i don't think so now you're saying it's they're doing intentionally i don't think they're doing intentionally but let let's let's let's it's an emergent phenomena okay yeah yeah yeah alright we'll get to that we'll get to that and so but but we wanted to see why so so we see what first as opposed so like because because it turns out that we're not the first to observe that legalese is weird like back to nixon had a plain language act in in nineteen seventy and and obama had one and boy a lot of these you know a lot of presidents have said oh we've gotta simplify legal language must simplify it but if you don't know how it's complicated it's not easy to simplify it you need to know what it is you're supposed to do before you can fix it right and so you need to like you need a psycholinguist to analyze the text and see what's wrong with it before you can like fix it you don't know how to fix it how am i supposed to fix something i don't know what's wrong with it and so what we did was just that's what we did we figured out let's look hey we just have a bunch of contracts had people and and we encoded them for the the it's a bunch of features and so another feature that people one of them was centrum betting and so that is like basically how often a a clause would in would would intervene between a subject and a verb for example that's one kind of a cent center embedding of a clause okay and turns out they're massively center embedded like so i think in random contracts and in random laws i think you get about seventy percent or eighty something like seventy percent of sentences have a sender embedded clause which is insanely high if you go to any other text it's down to twenty percent or something it's it's it's so much higher than the any control you can think of including you think oh people think oh technical academic text no people don't write central embedded sentences in in technical academic text i mean they do a little bit but much it's it's on the twenty percent thirty percent realm as opposed to seventy and so and so there's that and and there's low frequency words and then people oh maybe it's passive people don't like the passive passives for some reason the passive voice in english has a bad rap and i'm not really sure where that comes from and and there is a lot of passive in the there's much more passive voice in the in the in legalease than there is in in other types accounts for some of the low frequency words no no no no those are separate those are separate oh so passive voice sucks these these are low frequency words sucks what sucks is different so these are different judgment i'm yeah yeah yeah pass the drop the judgment it's just like these are frequent things which happen in legally text then we can ask yeah the dependent measure is like how well you understand those things with those features yes okay and so then and it turns out the passive makes no difference so it has a zero effect on your comprehension ability on your recall ability no nothing at all that has no effect your the the words matter a little bit they do and low frequency words are gonna hurt you in recall and understanding but what really what really hurts is the center embedded that kills you that is like that slows people down that makes them that makes them very poor at understanding that makes them they they they can't recall what was said as well nearly as well and we we did this not only on laypeople we did it on a lot of laypeople we ran it on a hundred lawyers we recruited lawyers from a from a wide range of of sort of different levels of law firms and stuff and they have the same pattern so they also like why when when they did this i did not know what happened i thought maybe they could process they're used to legalese they they process it just as well as if it was normal no no they they they're much better than laypeople so they're much like they can much better recall much better understanding but they have the same main effects as as as laypeople as laypeople exactly the same so they also much prefer the noncenter so we we we oh we constructed noncenter embedded versions of each of these mhmm we constructed versions which have higher frequency words in those places and we we did we un un un passivized we turned them into active versions the the passive active made no difference the words made a little difference and the un uncentered embedding made makes big differences in all the populations uncentered embedding how hard is that process by the way not very hard don't question but how hard is it to detect center embedding oh easy easy to detect yes you can do it long dependencies or is it real you can just you can so there's automatic parsers for english which are pretty good very good detect center point oh yeah very or oh i guess nest it yeah he he'd be large yeah pretty much so you you're not just looking for long dependencies you're just literally looking for center embed yeah we are in this case in this case but long dependencies are they're they're highly correlated these kinds of this so like a a center embedding is a is a big bomb you throw inside inside of a sentence that just blows up the that that makes yeah can i read a sentence for for you from these things i i i see what you can find i mean this is just like one of the things that this is just too my eyes might glaze over in middle mid sentence no i understand that i mean legal ease is here we go hard this is how they go because in the event that any payment or benefit by the company all such payments and benefits including the payments and benefits under section three a hereof being here at here and after referred to as a total payment would be subject to the excise tax then the cash severance payments shall be reduced so that's something we pulled from a regular text from a from a contract wow and and and the central embedded bit there is just for some reason there's a definition they throw the definition of what payments and benefits are in between the subject and the verb let's how about don't do that yeah how about put the definition somewhere else as opposed to in the middle of the sentence and so that's that's very very common by the way that's that's what happens you just throw your definitions you use a word couple words and then you define it and then you continue the sentence like just don't write like that and and you ask so when we ask lawyers we thought oh maybe lawyers like this lawyers don't like this they don't like this they don't wanna they don't wanna write like this they they we ask them to rate materials which are with the same meaning with with uncentrobed and centrobed and they much preferred the uncentrobed versions on the comprehension on the reading side yeah well the and and we asked them we asked them would you hire someone who writes like this or this we asked them all kinds of questions and they always preferred the less complicated version all of them so i don't even think they want it this way yeah but how did it happen how did it happen that's a very good question and and and the answer is i still don't know but i have some theories well our our best theory at the moment is that there's there's actually some kind of a performative meaning in the center embedding in the style which tells you it's legalese we think that that's the kind of style which tells you it's legalese like that's a it's a reasonable guess and maybe it's just so for instance if you're like it's like a magic spell so we kinda call this the magic spell hypothesis so when you give them when you tell someone to put a magic spell on someone what what do you do they you know people know what a magic spell is and they they do a lot of rhyming you know that's that's kinda what people will tend to do they'll do rhyming and they'll do sort of like some kind of poetry kinda of thing abracadabra type of thing yeah and maybe that's there's a syntactic sort of reflex here of a of a magic spell which is center embedding and so that's like oh it's trying to like tell you this is like this is something which is true which is what the goal of law law is right it's telling you something that we want you to believe as certainly true right that's that's what legal contracts are trying to enforce on you right and so maybe that's like a a form which has this is like an abstract very abstract form central embedding which has a has a has a meaning associated with it well don't you think there's an incentive yeah for lawyers to generate things that are hard to understand that was our one of our working hypotheses we just couldn't find any evidence of that no lawyers also don't understand it but we we we we space why you self but i mean you ask in a communist soviet union the individual members their self report is not going to correctly reflect what is broken about the gigantic bureaucracy that leads to chernobyl or something like this i think the incentives under which you operate are not always transparent to the members within that system yeah so like it just feels like a strange coincidence that like there is benefit if you just zoom out and look at the system as opposed to asking individual lawyers that make it something hard to understand is going to make a lot of people money yeah like there's going to you're gonna need a lawyer to figure that out i guess from the perspective of the individual but then that could be the performative asset it could it could be as opposed to the incentive driven to be complicated it could be performative to where we lawyers speak in this sophisticated way and you regular humans don't understand it so you need to hire a lawyer yeah i don't know which one it is but it's suspicious suspicious that it's hard to understand and that everybody's eyes glaze over and they don't read i i'm suspicious as well i'm still suspicious and i i hear what you're saying it could be kind of a you know no individual and even average of individuals it could just be a few bad apples in a way which are driving the effect in some way influential bad apples yeah at the sort of yeah that everybody looks up to whatever they're like is central figures in the in in how you know but it turns but it is it is kind of interesting fascinating among our hundred lawyers they did not share they didn't want this that's fascinating they really didn't like it and so it it takes time better at than regular people at comprehending it or they were they were on average better but they had the same difference the same same difference exact same difference so they but i they wanted it fixed so they they also and so that that gave us hope that because it actually isn't very hard to construct a a a material which is uncentered embedded and has the same meaning it's not very hard to do you just basically in that situation just putting definitions outside of the subject verb relation in that particular example and that's kind of that's pretty general what they're doing is just throwing stuff in there which you don't have to put in there there's extra words involved typically you may need a few extra words to sort of to refer to the things that you're defining outside in some way or as a because if you only use it in that one sentence then there's no reason to introduce extra extra terms but so we might have a few more words but it'll be easier to understand so i mean i i have hope that now that may maybe we can make legalese less less convoluted in the system the the next president of the united states can yeah instead of saying generic things say say exactly what ban center embeddings and make ted the the language czar of well you make eric okay martinez is the guy you should really put in there yeah yeah yeah i mean yeah but center beddings are the the the bad thing to have that's right so if you get rid of that that'll do a lot of it that'll fix a lot that's fascinating yeah that is so fascinating yeah and it there's really fascinating on many fronts that humans are just not able to deal with this kind of thing and that language because of that evolved in the way you did it's fascinating so one of the mathematical formulations you have when talking about language's communication is this idea of noisy channels what's a noisy channel oh so that that that's about communication and so this is going back to shannon so shannon claude shannon was a a student at mit in the forties and so he wrote this very influential piece of work about communication theory or information theory and he was interested in human language actually he was trying he was interested in this problem of communication of getting a a a message from my head to to your head and and so and he he was concerned or interested in what was a robust way to do that and so that yeah assuming we both speak the same language we both already speak english whatever you know whatever the language is we we speak that what is a way that i can say the language so that it's most likely to get the signal that i want to you and so and then the the the problem there in the communication is the noisy channel is that there's i make there's a lot of noise in the system i don't speak perfectly i make errors that's noise there's background noise you know you know that as we like a literal literal background noise there is like white noise in the background or some other kind of noise there's some speaking going on that you're or just that you're at a party that's background noise you you're trying to hear someone it's hard to understand them because there's all this other stuff going on in the background and and then there's noise on the communication on the on the receiver side so that you have some problem maybe understanding me for stuff that's just internal to you in some way so you've got some other problems whatever with understanding for whatever reasons maybe you're maybe you had too much to drink you know who knows why you're not able to pay attention to the signal so that's the noisy channel and so so that language if it's communication system we are trying to optimize in some sense the the passing of the message from one side to the other and so it turn i mean one idea is that maybe you know aspects of like word order for example might have optimized in some way to to make language a little more easy to be passed from speaker to listener and so shannon's the guy that did this stuff way back in the forties he you know it's very interesting though historically he was interested in working in linguistics he was in at mit and he did this is his master's thesis of all things you know it's crazy how much how much he did for his master's thesis in nineteen forty eight i think or forty nine something and and he wanted to keep working in language and it it just wasn't a popular communication as a as a reason a source for what language was wasn't popular at the time so chomsky was becoming was moving in there he was and he just wasn't able to get a handle there i think and so and so he moved to bellhops and worked on communication from a mathematical points point of view and was you know did all kinds of amazing work and so he's just more on the signal side versus like the language side yep hi it would have been interesting to see if you pursued the language side yeah that's really interesting yeah he was interested in that his examples in the forties are are are kinda like they're like very language like like things yeah we can kinda show that there's a noisy channel process going on in when you're listening to me you know you're you can often sort of guess what i meant by what i you know what what you think i meant given what i said and i i mean with respect to sort of why language looks the way it does we might we there might be sort of as i alluded to there might be ways in which word order is is somewhat optimized for for because of the noisy channel in some way i mean that's really cool to sort of model if you don't hear certain parts of a sentence or have some probability of missing that part like how do you construct a language that's resilient to that that's somewhat robust to that yeah that's the idea and then you're you're kinda saying like the word order and the the syntax of language the dependency length are all helpful yeah well dependency length is is really about memory really i think that's like about sort of what's easier or harder to produce in some way right and these other ideas are about sort of robustness to communication so the problem of potential loss of loss of signal due to noise and so that that so there may be aspects of word order which is somewhat optimized for that and and you know i we have this one guess in that direct and these are kinda just so stories i have to be you know pretty frank they're not like i can't show this is true all we can do is like look at the current languages of the world this is a like we can't sort of see how languages change or anything because we've got these snapshots of a few you know hundred or a few thousand languages we don't we don't really we we can't do the right kinds of modifications to test these these things experimentally and so you know so just take that this with a grain of salt okay from here this this stuff the dependency stuff i can i'm much more solid on i'm like here's what the lengths are and here's and here's what's hard and here's what's easy and this is a reasonable structure i think i'm pretty reasonable here's like why you know why does the word order look the way it does we're now into shaky territory but it's kinda cool but we're talking about just to be clear we're talking about maybe just actually the sounds of communicate like you and i are sitting in a bar it's very loud and you yeah you model with a noisy channel the loudness the noise and we have the signal that's coming across the and you're saying word order might have something to do with optimizing that when there's presence of noise yes yeah i mean it's really interesting i mean to me it's interesting how much you can load into the noisy channel like how much can you bake in well you said like you know cognitive load on the receiver end we think that those are there's three at least three different kinds of things going on there and we probably don't wanna treat them all as the same sure and so i think that you you know the right model a better model of a noisy channel would treat would have three different source sources of noise which because which are background noise you know speaker speaker inherent noise and listener inherent noise yeah and those are not the those are all different things sure but then underneath it it there's yeah a million other subsets of like what that's true on the on the receiving i mean i just mentioned cognitive load on both sides then there's like speaking speech impediments or just everything worldview i mean on the meaning we start to creep into the meaning realm of like we have different world views well how about just form still though like just just what language you know like so how well you know the language and so if it's second language for you versus first language and and how maybe what other languages you know these are still just form stuff and that's like potentially very informative and and you know how old you are these things probably matter right so like a child learning a language is is a you know is a noisy representation of english grammar you know depending on how old they are so maybe when they're six they're perfectly formed but you mentioned one of the things is like a way to measure the the a language is learning problems so like what's the correlation between everything we've been talking about and how easy it is to learn a language so is is like a short dependencies correlated to ability to learn a language is there some kinda or like the dependency grammar is there some kinda connection there how easy it is to learn yeah well all the languages in the world's language none is right now we know is any better than any other with respect to sort of optimizing dependency lengths for example they're all kind of do it do it well they all keep low it's so the i i think of every human language as some kind of an sort of an optimization problem a complex optimization problem to this communication problem and so so they've like they've solved it and you know they're just sort of noisy solutions to this problem of communication there's just so many ways you can do this so they're not optimized for learning they're probably optimized for communication and and learning so yes one of the factors which is uh-oh yeah so learning is messing this up a bit and so so for example if it were just about minimizing dependency lengths and and that was all that matters you know then we you know so then then we might find grammars which didn't have regularity in their rules like but languages always have regularity in their rules so so what i mean by that is that if if i wanted to say something to you in the in the optimal way to say it was in what really mattered to me all that mattered was keeping the dependencies as close together as possible then i then i would have a very lax set of phrase structure rule or or dependency rule i wouldn't have very many of those i would have very little of that and i would just put the words as close to the things that refer to the things that are connected right beside each other but we don't do that mhmm like there are like there there are word order rules right so they're very and and depending on the language they're more and less strict right so you speak russian they're less strict than english english has very rigid word order rules we order things in a very particular way and and so why do we do that like that's probably not about communication that's probably about learning i mean then we're talking about learning it's probably easier to learn regular regular things things which are very predictable and easy to so so that's that's probably about learning is my is our guess because that can't be about communication be just noise can it be just the the messiness of the development of a language well if it were just a communication then we we should have languages which have very very free word order and we don't have we have freer but not free like there's always well no but what i mean by noise is like cultural like sticky cultural things like the way the way you communicate just there there's a stickiness to it that it's it's an imperfect mhmm it's a noisy stochastic yeah the the the function over which you're optimizing is very noisy yeah so because i don't it feels weird to say that learning is part of the objective function because some languages are way harder to learn than others right or is that that's interesting i mean i mean that's the public sort of perception right yes that's true the for a second language for a second language but that depends on what you started with right so so it's it really depends on how close that second language is to the first language you've got and so yes it's very very hard to learn arabic if you've started with english or it's harder to you know hard to learn japanese or or if you've started with chinese i think is the worst in the there's like defense language institute in the u in the united states has like a a list of of of how hard it is to learn what language from english i think chinese is the first the second lang i see you're saying babies don't care no there's no evidence that there's anything harder easier but any baby any language learned like by three or four they speak that language and so there's no evidence of any anything harder or easier about any human language they're all kinda equal to what degree is language this is returning to chomsky a little bit is is innate you said that for chomsky he used the idea that language is some aspect of language or a need to explain away certain things that are observed and do how much are we born with language at the core of our mind brain i mean i i you know the answer is i don't know of course but the i mean i i like to i'm an engineer at heart i guess and i sort of think it's fine to postulate that a lot of it's learned and so i i'm guessing that a lot of it's learned so i think the reason chomsky went with innateness is because he he he hypothesized movement in his grammar he was interested in grammar and movement's hard to learn i think he's right movement is a harder it's a hard thing to learn to learn these two things together and how they interact and there's like a lot of ways in which you might generate exactly the same sentences and it's like really hard and so he's like oh i guess it's learned sorry so i guess it's not learned it's innate mhmm and if you just throw out the movement and just think about that in a different way you you know then you you get some messiness but the messiness is human language which it it actually fits better it's a that messiness isn't a problem it's actually a it's a valuable asset of of of the theory and so so i think i don't really see a reason to postulate much much innate structure and that's kind of why i think these large language models are learning so well mhmm is because i think you can learn the form the forms of human language from the input i think that's like it's likely to be true so that part of the brain that lights up when doing all the comprehension that could be learned that could be just yeah you don't need you don't need eight have to be an eight so like lots of stuff is modular in the brain that's learned it doesn't have to you know so there's something called the visual word form area in the back and so it's in the back of your head near the you know the visual cortex okay and that is very specialized language sorry very specialized brain area which does visual word processing if you read if you're a okay guess what you spend some time learning to read and you develop that that brain area which does exactly that and so these the modularization is not evidence for innateness so the modularization of a language area doesn't mean we're born with it we could have easily learned that i i we might have been born with it i i we we just don't know at this point we might very well have been born with this left lateralized area i mean there's like a lot of other interesting components here features of this kind of argument so some people get a stroke or something goes really wrong on the left side where the left where the language area would be and that and that isn't there it's not not available and it develops just fine in the right so it's no it's so it's not about the left it it it goes to the left like this is a very interesting question it's like why is the why are any of the brain areas the way that they are and how how how did they come to be that way and you know there's these natural experiments which happen where people get these you know strange events in their brains at very young ages which wipe out sections of their brain and and they behave totally normally and no one knows anything was wrong and we find out later because they happen to be accidentally scanned for some reason it's like what what happened to your left hemisphere it's missing there's not many people who've missed their whole left hemisphere but they'll be missing some other section of their left or their right and they behave absolutely normal we would never know so that's like a very interesting you know current research you know this is another project that this person em fedorenko is working on she's got all these people contacting her because she's scanned some people who have been missing sections one person missing missed a section of her brain and was scanned in her lab and and she and she happened to be a writer for the new york times and there was an article in the new york times about about the just about the scanning procedure and and about what might be learned about by sort of the general process of mri and language and then up with this or language and and because she's writing for the new york times and all these people started writing to her but who also have similar similar kinds of deficits because they've been you know accidentally you know scanned for some reason and and found out they're missing some section they they volunteer to be scanned so these are natural experiments natural experiments they're kinda messy but natural experiments it's kinda cool the the the she calls them interesting brains the first few hours days months of human life are fascinating mhmm it's like yeah well inside the womb actually like that development that machinery whatever that is seems to create powerful humans that are able to speak comprehend think all that kind of stuff no matter what happened not no matter what but robust to the different ways that the the the brain might be damaged and so on that that's really that's really interesting but what what would chomsky say about the fact the thing you're saying now that language is is seems to be happening separate from thought because as far as i understand maybe you can correct me he thought that language underpins yeah he thinks so i don't know what he'd say he would be surprised because for him the idea is that language is the sort of the foundation of thought that's right absolutely and it's pretty mind blowing to think that it could be completely separate from thought but so you know he's basically a philosopher philosopher of language in a way thinking about these things it's a fine thought you can't test it in his methods you can't do do a thought experiment to figure that out you need a scanner you need brain damaged people you need something you need ways to to measure that and that's what you know fmri offers as a and and and you know patients are a little messier fmri is pretty unambiguous i'd say it's like very unambiguous there's no way to say that the language network is doing any of these tasks there's like you you should look at those data it's like there's no chance that you can say that that there those networks are overlapping they're not overlapping they're just like completely different and so you know so the the you know you can always make you know it's only two people it's four people or something for the the patients and there's something special about them we don't know but these are just random people mhmm and and with lots of them and you find always the same effects and it's very robust i'd say what's the fascinating effect what's the you mentioned bolivia mhmm what's the connection between culture and language you've you've also mentioned that you know much of our study of language comes from w e i r d weird people western educated industrialized rich and democratic so when you study like remote cultures such as around the amazon jungle what can you learn about language so that term weird is from joe henrich he's at harvard he's a harvard evolutionary biologist and so he works on lots of different topics and he basically was pushing that observation that we should be careful about the inferences we wanna make when we're talk in psychology or socio yeah mostly in psychology i guess about humans if we're talking about you know undergrads at mit and harvard those aren't the same right these aren't the same things and so if you wanna make inferences about language for instance you there's a lot of very a lot of other kinds of languages in the world than english and french and chinese you know and so maybe in for for for language we care about how culture because cultures can be very i mean of course english and chinese cultures are very different but in in you know hunter gatherers are much more different in in some ways and so you know if culture has an effect on what language is then we kinda wanna look there as well as looking it's not like the industrialized cultures aren't interesting of course they are but we want to look at non industrialized cultures as well and so i i've worked with two i've worked with the tsimane which are in bolivia and and the amazon both in the amazon in these cases and the there are so called farmer foragers which is not hunter gatherers it's sort of one up from hunter gatherers in that they do a little bit of farming as well a lot of hunting as well but a little bit of farming and the and the kind of farming they do is the kind of farming that i might do if i ever were to grow like tomatoes or something in my backyard it's it's that it's not like so it's not like big field farming it's just a farming for a family a few things you do that and so that's what that's the kind of farming they do and the other group i've worked with are the piraha which are in also in the amazon and happened to be in brazil and that's with a guy called dan everett mhmm who was a linguist anthropologist who actually lived and worked in the i mean he was a missionary actually initially back in the seventies working with trying to translate languages so they could teach them the bible teach them christianity what what can you say about that yeah so the two groups i've worked with the chimani and the piraha are both isolate languages meaning there's no known connected languages at all they're just like on their own cool yeah there's a lot of those and and most of the isolates occur in in the in the amazon or in papua new guinea and these these places where the world has sort of stayed still for long enough and there like so there there aren't earthquakes there aren't well certainly no earthquakes in the amazon jungle and and and the the climate isn't bad so you don't have droughts and so you know in africa you've got a lot of moving of people because there's drought problems and so so they get a lot of language contact when you have when people have to if you gotta move because you're you've got no water then you've gotta get going and then then you run into contact with other other tribes other groups in in the amazon that's not the case and so people can stay there for hundreds and hundreds and probably thousands of years i guess and so these groups have and the the chimani and the pinahan are both isolates in that and they can just i guess they've just lived there for ages and ages with minimal contact with other outside groups and so i i mean i'm interested in them because they are i mean i i you know in this case i'm interested in their words i mean i would love to study their syntax their orders of words but i'm mostly just interested in how languages you know are connected to their their cultures in this way and so with the piraha sort of most interesting i was work i was working on number there number information and so the the basic idea is i think language is invented right that's what i get from the words here is that i think language is invented we talked about color earlier it's the same idea so that what you need to talk about with someone else is what you're gonna invent words for okay and so we invent labels for colors that i need not that i that i can see but that but that things i need to tell you about so that i can get objects from you or get you to give me the right objects and i just don't need a word for teal or or a word for aquamarine in in the in the amazon jungle for the most part because i don't have two things which differ on those colors i just don't have that and so and so numbers are really another fascinating info source of information here where you might i mean naively i certainly thought that all humans would have words for exact counting mhmm and the piraha don't okay so they don't have any words for even one there's not a word for one in their in their language and so there's still no word for two three or four so so that kind of blows people's minds off and so that's blowing my mind that's pretty weird how are you how are you gonna ask i want two of those you just don't and so that's just not a thing you can possibly ask in the funeral it's not possible that is there's no words for that so here's how we found this out okay so so it was thought to be a one two many language there are three words for quantifiers for for for sets but and and the people had thought that those meant one two and many mhmm but what they really mean is few some and many many is correct it's few some and many and so and so the way we we figured this out nice and this is kinda cool is that we gave people we had a set of objects okay and these were happen to be spools of thread doesn't really matter what they are identical objects and and and when i sort of start off here i just give you know give you one of those and say what's that okay so you're a piano speaker and you tell me what it is and and then i give you two and say what's that and and nothing's changing in this set except for the number okay and then i just ask you to label these things we just do this for a bunch of different people and and frankly it's a i i did this task this is fascinating and it's a weird it's a little bit weird so you they say the word that they thought that we thought was one it's few but for the first one and then maybe they say few or maybe they say some for the second and then for the third or the fourth they start using the word many for the set and then five six seven eight i go all the way to ten and and and it's always the same word and they look at me like i'm stupid because they told me what the word was for six seven eight and i'm gonna continue asking them at nine and ten i'm not like i'm sorry i just i just they understand that i wanna know their language that's the point of the task is like i'm trying to learn their language and so that's okay but it does seem like i'm a little slow when because i they already told me what the word for many was five six seven and i keep asking so it's a little funny to do this task over and over we did this with the guy called dan was the our translator he's the only one who really speaks piraha fluently he's a good you know bilingual for a bunch of languages but also in english and and and then a guy called mike frank was also a student with me down there he he and i did these things and so you do that okay and everyone does the same thing they all all all you know and we ask like ten people and they all do exactly the same labeling for one up and then we just do the same thing down on like random order actually we do some of them up some of them down first okay and so we do instead of one to ten we do ten down to one and so so i give them ten nine and eight they start saying the word for some and then at down when you get to four everyone is saying the word for few which we thought was one so it's so like it's the context determined what word what what what that quantifier they used was so it's not a count word they're not they're not count words they're they're just approximate words and they're gonna be noisy when you interview a bunch of people the what the definition of few and there's gonna be a threshold in the context yeah yeah con yeah i don't know what that means that's that's gonna depend on the context so we i think it's true in english too right if you ask an english person what a few is i mean that's dependent completely on the context and it might actually be at first hard to discover yeah because for a lot of people the jump from one to two will be few right so it's a jump yeah yeah it might be it might still be there yeah right it's i mean that's fascinating that's fascinating the numbers don't present themselves like yeah so the words aren't there and then and so then we do these other things well if if they don't have the words can they do exact matching kinds of tasks can they even do those tasks and and and the answer is sort of yes and no and so yes they can do them so here's the tasks that we did we we put out those spools of thread again okay so maybe i put like three out here and then we gave them some objects and those happen to be uninflated red balloons it doesn't really matter what they are it's just they're a bunch of exactly the same thing and it was easy to put down right next to these spools of threat okay and so then i put out three of these and your task was to just put one against each of my three things and they could do that perfectly so i mean i would actually do that it was a very easy task to explain to them because i have i did this with this guy mike frank and he would be my i i'd be the experimenter telling him to do this and showing him to do this and then we just like just do what he did you'll copy him all we had to i didn't have to speak puree except for know what copy him like do what he did is like all we had to be able to say and and then they would do that just perfectly and and so we'd move it up we'd do some sort of random number of items up to ten and they basically do perfectly on that they never get that wrong i mean that's not a counting task right that is just a match you just put one against it doesn't matter how many i don't need to know how many there are there to do that correctly and and they would make mistakes but very very few and no more than mit undergrads just gonna say like there's there's no these are low stakes so you know you make mistakes is not required to complete the matching test not not at all okay and so and and so that's our control and this guy a guy had gone down there before and said that they couldn't do this task but i just don't know what he did wrong there because they can do this task perfectly well and you know i can can train my dog to do this task so of course they can do this task and so you know it's not a hard task but the other task that was sort of more interesting is like so then we do a bunch of tasks where you need some way to encode the set so like one one of them is just i i've just put a a a opaque sheet in front of the of the things i put down a bunch a set of these things and i put an opaque sheet down and so you can't see them anymore and i tell you do the same thing you were doing before right you you know and it's easy if it's two or three it's very easy but if i don't have the words for eight it's a little harder like maybe you know with practice when well no because you have to count for us it's easy because we just we just count them it's just so easy to count them but but they don't they can't count them because they don't count they don't have words for this thing and so they would do approximate it's totally fascinating so they would get them approximately right you know you know after four or five you know because you can basically you always get four right three or four that looks that's something we can visually see but but after that you kinda have it's a approximate number and so then and there's a bunch of tasks we did and they all failed as i mean failed they did approximate after five on all those tasks and it just kinda shows that the words you kinda need the words you know to be able to do these these kinds of tasks there's a little bit of a chicken and egg thing there because if you don't have the words then maybe they'll limit you in the kind of like a little baby einstein there won't be able to come up with a counting task you know what i mean like the ability to count enables you to come up with interesting things probably mhmm so yes you develop counting because you need it but then once you have counting you can probably come up with a bunch of different inventions mhmm like how to i don't know what kind of thing they do matching really well for building purposes building some kinda hut or something like this mhmm so it's interesting that language is a limiter on what you're able to do yeah here language is just is the words here is the words like the words for exact count is the limiting factor here they just don't have them yeah in this yeah well that that's what i mean yeah yeah the that limit mhmm is also a limit on the society of what they're able to build that's gonna be true yeah so it's prob i mean we don't know this is one of those problems with the snapshot of just current languages is that we don't know what causes a culture to discover slash invent accounting system but the hypothesis is the guess out there is something to do with farming so if you have a bunch of goats and you wanna keep track of them and you have saved seventeen goats and you go to bed at night and you get up in the morning boy it's easier to have a count system to do that you know if i have and that's an abstract an abstraction over a set so that i don't have like people often ask me when i talk to them about this kind of work and they say well don't these peter hahn don't they have kids don't they have a lot of children i'm like yeah they have a lot of children and they do they they often have families of three or four or five kids and and they go well don't they need the numbers to keep track of their kids and i and i always ask this person who says this like do you have children and the answer is always no because that's not how you keep track of your kids yeah you you you care about their identities it's very important to me when i go i think i have five children it's it's it's it's it doesn't matter which yeah doesn't it matters which five it's like if you replaced one with someone else i would i would care goat maybe not right that's the kind of point it's an abstraction something that looks very similar to the one wouldn't matter to me probably but if you care about goats you're gonna know them actually individually also yeah you will i mean cows goats if there's a source of food and milk and all that kind of stuff you're gonna actually you're absolutely right but but i'm saying it is an abstraction such that you don't have to care about their identities to do this thing fast that's that's the hypothesis not mine from anthropologists as they're guessing about where words for counting came from is from farming maybe anyway do you have a sense why universal languages like esperanto have not taken off like why do we have all these different languages yeah yeah well my guess is the the the function of a language is to do something in a community and and i mean unless there's some function to that language in the community it's it's not gonna survive it's not gonna be useful so here's a great example so what i'm like language death is super common okay languages are dying all around the world and here's how here's why they're dying and it's like yeah i see this in you know in it's not happening right now in either the chimane or the or the pirahan but it it probably will and so there's a neighboring group called mosetan mhmm which is i i i i said that it's isolated it's actually there's a dual there's two of them okay so it's act there's two languages which are really close which are moesitan and and tsimane which are unrelated to anything else and moesitan is unlike chimane in that it has a lot of contact with spanish and it's dying so that language is dying the reason it's dying is there's not a lot of value for the local people in their native language so there's much more value in knowing spanish like because they wanna feed their families and how do you feed your family you learn spanish so you can make money so you can get a job and do these things and then you can and then you make money and so they want spanish things they want and so so most of them is staying is in danger and is dying and that's normal and so basically the problem is that people like the reason we learn language is to communicate and and we need to we use it to to make money and to do whatever it is to to feed our families and if that's not happening then it it won't take off it's not like a game or something this is like something we like why is english so popular it's it's not because it's an easy language to learn maybe it is i don't really know it it's but that's not why it's popular but because it's a the united states is a gigantic economy therefore it's big economies that do this it's all it is it's all about money and that that's what and so you know there's there's a motivation to learn mandarin there's a motivation to learn spanish there's a motivation to learn english these languages are very valuable to know because there's so so many speakers all over the world that's fascinating there's less of a value economically it's like kind of what drives this it's not about it's not a you know it's not just for fun i mean there are these groups that do want to learn language just for language's sake and they want and then and there's something you know to that but those are rare those are rarities in general those are a few small groups that do that not most people don't do that well if that was the primary driver then everybody was speaking english or speaking one language there's also attention that's happening that well well that towards fewer and fewer languages exactly yeah i wonder if you're right maybe maybe you know this is slow yeah but maybe that's where we're moving but there is a tension you're saying language is at the fringes but if you look at geopolitics and superpowers it does seem that there's another thing in tension which is a a language is a national identity sometimes oh yeah a certain nation i mean that's the the war in ukraine language ukrainian language is a symbol of that war in many ways like a country fighting for its own identity so it's not merely the convenience i mean those two things are attention it's the the convenience of trade and the economics and be able to communicate with neighboring countries and trade more efficiently with neighboring countries all that kind of stuff but also identity that's right of the group i completely agree because language is the way for every community like dialects that emerge are a kind of identity for people yeah sometimes a way for people to say f u to the more powerful yeah people and it's interesting so in that way language can't be used as that tool yeah it it it i completely agree and there's a lot of work to try to create that identity so people want to do that speak you know as a cognitive scientist and language expert i i i hope that continues because i don't want languages to die i want languages to survive because i because they're so interesting for for so many reasons but i mean i i i find them fascinating just for the language part but i think they you know there's a lot of connections to culture as well which is also very important do you have hope for machine translation that can break down the barriers of language so while all these different diverse languages exist i guess there's many ways of asking this question but basically how hard is to it to translate in an automated way from one language to another there's there's gonna be cases where it's gonna be really hard right so there are concepts that are in one language and not in another like the most extreme kinds of cases are these cases of number information so exact like good luck translating a lot of english into pira it's just impossible there's no way to do it because there are no words for these concepts that we're talking about there's probably the the flip side right there's probably stuff in which is gonna be hard to translate into english on the other side and so i just don't know what those concepts are i mean you know the the space the world space is a little is different from my world space and so i don't know what like so that the things they talk about things are you know it's gonna have to do with their life as opposed to you know my industrial life which is gonna be different and and so there's gonna be problems like that always you know there's like it's not maybe it's not so bad in the case of some of these spaces and maybe it's gonna be harder than others and so it's pretty bad in number it's like you know extreme i'd say in the number space you know exact number space but in the in the color dimension right so that's not so bad there's i mean but it's it's a problem that that you don't have ways to talk about the concepts there's and there might be entire concepts that are missing mhmm so to you it's more about the space of concept versus the space of form like form you can probably map yes yeah but so you were talking earlier about translation and about how translations you know there's good and bad translations i mean now we're talking about translations of form right so what makes writing good right you know it's not just music in the form right it's it's not just the content it's you know it's how it's written and translating that i you know i you know that's that sounds difficult so then we should we should say that there is like i don't i hesitate to say meaning but there's a music and a rhythm to the form mhmm when you look at the broad picture like yeah difference between dostoevsky and tolstoy or heming you know hemingway bukowski james joyce like i mentioned there's a beat to it there's an edge to it that it's like is in the form we can probably get measures of those yeah i i i don't know that's interesting optimistic that we could get measures of those things and so maybe that's math --atable i don't know i don't know though i i have not worked on that i would love to see translation fascinating to hemi i mean hemiway is probably the lowest i would love to do see different authors but the average per sentence dependency length for hemingway is probably the shortest that's your sense it's simple sentences with short yeah short yeah yeah yeah yeah yeah i mean that's when if you have really long sentences even if they don't have center embedding like they can have longer connections yeah they can have longer connections they don't have to right you can have a long long sentence with a bunch of local words yeah yeah but it's but it is much more likely to have the possibility of long dependencies with long sentences yeah i met a guy named azaraskin who who does a lot of cool stuff really brilliant works with tristan harris and a bunch of stuff but he was talking to me about communicating with animals he cofounded earth species project where you're trying to find the common language between whales crows and humans mhmm and he was saying that there is a there's a lot of promising work mhmm that even though the signals are very different right like the actual like if you have embeddings of the languages they're actually trying to communicate similar type things mhmm and is there something you can comment on that like where is there a promise to that in in everything you've seen in different cultures especially like remote cultures yeah that this is a possibility or no that we can talk to whales i i i would say yes i i i think it's not crazy at all i think it's quite reasonable there there's this sort of weird view well odd view i think that to think that human language is somehow special i mean it is maybe it is we can certainly do more than any of the other species you know we you know and so and maybe maybe our language system is part of that it's possible but but then but people do have often talked about how human like chomsky in fact is talk about how human only only human language has you know this you know this this compositionality thing that he thinks is sort of key in in language and it's the the problem with that argument is he doesn't speak whale yeah and he doesn't speak crow and he doesn't speak monkey you know he's like they they say things like well they're making a bunch of grunts and squeaks and and and that their reasoning is like that's bad reasoning like you know i'm pretty sure if you asked a whale what we're saying they'd say well i'm making a bunch of weird noises exactly and so it's like this is a very odd reasoning to to be making that human language is special because we're the only ones who have human language i'm like well we don't know what those other we just don't know we can't talk to them yet and so there probably is signal in there and it might very well be something complicated like human language i mean sure with a small brain in in in lower in lower species there's probably not a very good communication system but in these higher higher species where you have you know what seems to be you know abilities to communicate something there might very well be a lot more signal there than we're than than we might have otherwise thought but but also if we have a lot of intellectual humility here there's somebody formerly from mit nari oxman who i admire very much has talked a lot about has worked on communicating with plants mhmm so like yes the signal there is even less than mhmm but like it's not out of the realm of possibility mhmm that all nature has a way of communicating and it's a very different language but they do develop a kind of language through the chemistry through some way of communicating with each other and if you have enough humility about that possibility i think you can i think it would be a very interesting in a few decades maybe centuries hopefully not a humbling possibility of being able to communicate not just just between humans effectively but between all of living things on earth well i mean i think some of them are not gonna have much interesting to say but you could still we don't know we certainly don't know i think i i think if we're humble yeah there could be some interesting trees out there oh yeah yeah yeah yeah well they're probably talking to other trees right they're not talking to us and so the the to the extent they're talking they're saying something interesting to some other you know you know conspecific as opposed to us right and so there probably is there may be some signal there i i you know so there are people out there actually it's pretty common to say that lang that human language is special and different from any other animal communication system and i i i just i just don't think the evidence is there for that claim i think it's not obvious you know i do we just don't know what what because we we we don't speak these other communication systems until we get better you know i i do think there's there are people working on that as you pointed out though people working on whale speak for instance like that's really fascinating let me ask you a wild out there sci fi question if we make contact with an intelligent alien civilization and you get to meet them mhmm how hard do you think it like how surprised would you be about their way of communicating do you think you would it'd be recognizable maybe there's some parallels here to when you go to the remote drives i mean i i would want dan everett with me he is like amazing at learning foreign languages and so he like this is an amazing feat right to be able to go this is a language piraha which has no translators before him i i mean there were wow he was just sharing with it well there was a guy that had been there before but he wasn't very good yeah and so he learned the language far better than anyone else had learned before him he's like good at he's just a he's a very social person i think that's a big part of it is being able to interact so i don't know it kinda depends on these these this this species from outer from outer space how how much they wanna talk to us is there something you can say about the process he follows like what how do you show up to a tribe and socialize i mean i guess colors and counting is a is one of the most basic thing to figure out yeah you start that you actually start with like objects yes and just say you know just throw a stick down and say stick and then you say what do you call this and they do this feature and then they'll say the word whatever and he says the standard thing to do is to throw two sticks at two sticks and then you know he learned pretty quick that there weren't any count words in this language because they didn't know this wasn't interesting to i mean it was kinda weird they'd say some or something the same word over and over again and so but that is a standard thing you just like try to but you have to be pretty out there socially like willing to talk to random people which these are you know really very different people from you and he was and he's he's very social and so i think that's a big part of this is like that's how you know a lot of people know a lot of languages is they're willing to talk to other people that's a tough one where you just show up knowing nothing yeah oh god yeah it's a beaut it's beautiful that humans are able to connect in that way yeah yeah you've had an incredible career exploring this fascinating topic what advice would you give to young people about how to have a career like that or a life that that that they can be proud of when you see something interesting just go and do it like i do i do that like i that's something i do which is kind of unusual for most people so like when i saw the like if peter rahn was available to go and visit i was like yes yes i'll go yeah and then when we couldn't go back we had some trouble with the brazilian government there's some corrupt people there it was very difficult to get go back in there and so i was like alright i gotta find another group and so we searched around and we're able to find the because i wanted to keep working on this kind of problem and so we found the tsimane and just go there i didn't really have we didn't have contact we had a little bit of contact and brought someone and that and that was you know we just you just kinda just try things i i say it's like a lot of that just like ambition just try to do something that other people haven't done just give it a shot is what i i mean i i do that all the time i don't know i love it but and i love the fact that your pursuit of fun has landed you here talking to me this was an incredible human being thank you for taking a journey through human language with me today this is awesome thank you very much alexis my pleasure thanks for listening to this conversation with edward gibson to support this podcast please check out our sponsors in the description and now let me leave you with some words from wittgenstein the limits of my language mean the limits of my world thank you for listening and hope to see you next time